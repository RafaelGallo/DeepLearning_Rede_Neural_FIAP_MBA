# -*- coding: utf-8 -*-
"""RedeNeural_previsao_açoes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17QSDGOW-Me13oU7cfFAKqqRomaPRLYWq

# **QuantumFinance – Deep Learning para Decisões no Mercado Financeiro**

![](https://sdmntprsouthcentralus.oaiusercontent.com/files/00000000-2b74-61f7-bb25-a7f51333e8fc/raw?se=2025-06-30T00%3A52%3A43Z&sp=r&sv=2024-08-04&sr=b&scid=c2cd711a-1e4b-5252-b0dd-1e6a54bb67a8&skoid=9ccea605-1409-4478-82eb-9c83b25dc1b0&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-06-29T19%3A12%3A01Z&ske=2025-06-30T19%3A12%3A01Z&sks=b&skv=2024-08-04&sig=SxrwCr93Ox23UayGCGxdXIT4mf8yn5Ba%2B9xwJMY2Nc8%3D)

# **Parte 1 - Problema de Negócio**

### Contexto

A QuantumFinance, uma gestora de recursos especializada em inovação e tecnologia, está planejando lançar um fundo de ações 100% baseado em **modelos de Deep Learning**. O objetivo é construir um sistema automatizado de compra e venda de ações, buscando rentabilidade superior ao mercado usando inteligência artificial.

Para iniciar este projeto, foram selecionadas **quatro ações brasileiras altamente negociadas**:

* **VALE3** — Vale S.A. (Mineração)
* **PETR4** — Petrobras (Petróleo & Energia)
* **BBAS3** — Banco do Brasil (Financeiro)
* **CSNA3** — Companhia Siderúrgica Nacional (Siderurgia)

### Objetivo do Projeto

O objetivo central é **desenvolver modelos de Deep Learning capazes de identificar tendências de alta e baixa** no preço dessas ações, automatizando a decisão de **comprar** ou **vender** cada papel diariamente. O foco é construir um “**perseguidor de tendência**”:

* **Compra** quando o papel está em tendência de alta
* **Venda** quando o papel está em tendência de baixa

Essas decisões devem ser tomadas **automaticamente** com base apenas nos dados históricos recentes do mercado.

### Detalhes do Desafio

#### 1. **Rotulagem & Dados**

* Cada ação possui dados históricos diários de preços (de 2000 a 2023).
* Especialistas rotularam, dia a dia, se a ação deveria ser **comprada** (+1) ou **vendida** (-1) no dia seguinte, considerando a tendência dos últimos 15 dias.
* Os dados são fornecidos em dois formatos:

  * **Tabular (CSV)**: Cada linha contém a data, preço de fechamento filtrado, rótulo de compra/venda e as séries dos 15 dias anteriores.
  * **Imagens (PNG)**: Gráficos de barras normalizados dos últimos 15 dias para cada dia (permitem uso de CNN 2D).

#### 2. **Premissas**

* A estratégia do fundo depende da precisão dos modelos para identificar corretamente as tendências.
* Todas as decisões precisam ser baseadas **apenas em informações disponíveis até o dia anterior** (problema real de predição/robustez).

#### 3. **Requisitos do Projeto**

* Treinar **modelos de Deep Learning** para cada ação.

  * Sugerido: CNN 1D/2D, RNN (LSTM/GRU), modelos híbridos, etc.
* O sistema deve indicar diariamente: **COMPRAR** (+1) ou **VENDER** (-1).
* O modelo precisa ser entregue em **Jupyter Notebook**, incluindo:

  * **Acurácia** no teste.
  * **Matriz de confusão**, **precision**, **recall**.
  * **Backtest financeiro**: simulação do lucro se o modelo fosse usado para operar de fato.

#### 4. **Avaliação do Projeto**

* **Resultados quantitativos**: métricas de classificação e desempenho financeiro (lucro, drawdown, etc.).
* **Diferenciais inovadores** são incentivados: uso de arquiteturas avançadas, ensembles, integração de imagens com dados tabulares, explainability, dashboards interativos.
*
### Motivação de Negócio

* **Automatização inteligente** das operações para aumentar a eficiência e potencializar os lucros do fundo.
* **Redução do viés humano**, permitindo decisões consistentes e baseadas em dados.
* **Adoção de IA de ponta** para construir um diferencial competitivo no setor financeiro.

### Exemplos de Questões a serem Respondidas

* O modelo consegue aprender padrões de tendência nos preços das ações?
* O sistema realmente gera lucro financeiro consistente ao longo do tempo?
* Como o desempenho do modelo se compara a estratégias simples (ex: buy & hold)?
* É possível aprimorar o modelo integrando múltiplos tipos de entrada (dados tabulares + imagens)?

## Tecnologias e Abordagens Utilizadas

### 1. **Redes Neurais para Análise de Séries Temporais e Padrões Visuais**

Para resolver o desafio proposto, serão empregadas as seguintes tecnologias de Deep Learning, escolhidas por sua capacidade de identificar padrões complexos em séries históricas e dados visuais:

#### **a) LSTM (Long Short-Term Memory)**

* **O que é:** Uma arquitetura de rede neural recorrente (RNN) capaz de aprender dependências de longo prazo em sequências temporais.
* **Aplicação no projeto:** Permite que o modelo “lembre” dos movimentos dos preços ao longo dos últimos 15 dias, capturando tendências e reações típicas do mercado.
* **Benefício para o negócio:** Maior precisão na detecção de tendências e pontos de reversão, aumentando o potencial de acerto das operações.

#### **b) GRU (Gated Recurrent Unit)**

* **O que é:** Variante simplificada da LSTM, com desempenho semelhante e treinamento mais rápido em alguns cenários.
* **Aplicação no projeto:** Ótima opção para testar sequências temporais, podendo ser combinada em modelos híbridos.
* **Benefício para o negócio:** Eficiência computacional e boa capacidade preditiva, acelerando experimentação e implantação.

#### **c) CNN 1D (Convolutional Neural Network 1D)**

* **O que é:** Rede neural que aprende padrões locais em sequências temporais (por exemplo, saltos bruscos, reversões ou padrões recorrentes nos preços).
* **Aplicação no projeto:** Explora comportamentos específicos dentro da janela de 15 dias.
* **Benefício para o negócio:** Complementa as redes recorrentes, identificando sinais rápidos ou mudanças abruptas que possam indicar oportunidades de trade.

#### **d) CNN 2D (Convolutional Neural Network 2D)**

* **O que é:** Rede neural poderosa para análise de imagens.
* **Aplicação no projeto:** Usada para analisar gráficos PNG dos últimos 15 dias, permitindo ao modelo capturar padrões visuais que especialistas humanos muitas vezes identificam visualmente.
* **Benefício para o negócio:** Abre espaço para inovação, aproveitando representações visuais dos dados para melhorar a tomada de decisão.

### 2. **Análise de Dados e Pipeline de Modelagem**

* **Pré-processamento:** Limpeza, normalização/padronização de dados e organização de janelas deslizantes para os modelos sequenciais.
* **Feature Engineering:** Extração automática de padrões e relações temporais/pictóricas relevantes para previsão.
* **Avaliação Rigorosa:** Utilização de métricas clássicas (acurácia, precisão, recall, matriz de confusão) e **backtest financeiro** simulando operações reais.
* **Experimentação com Híbridos e Ensembles:** Combinação dos melhores modelos para potencializar resultados.
* **Explainability:** Análise de importância das features e visualização das ativações para auditoria do comportamento dos modelos.
* **Implementação em Jupyter Notebook:** Documentação, reprodutibilidade e transparência.

## Resumindo:

> A QuantumFinance utilizará o que há de mais moderno em inteligência artificial aplicada ao mercado financeiro, integrando redes neurais especializadas (LSTM, GRU, CNN 1D e 2D) e técnicas avançadas de análise de dados para construir um sistema robusto, inteligente e voltado para resultados reais.

### Impacto Esperado

* **Se bem-sucedido, o modelo pode ser usado como base para estratégias reais de investimento**, aumentando a rentabilidade dos fundos da QuantumFinance e abrindo portas para novos produtos baseados em IA.

## Resumão Visual do Problema

> **Problema:**

> Prever diariamente, com base nos últimos 15 dias, se cada ação selecionada deve ser comprada ou vendida, usando Deep Learning, maximizando o lucro do fundo via automação de decisões.

# **Parte 2 - Importando bibliotecas**
"""

import os
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

"""# **Parte 3 - Base dados**"""

# Base dados - BBAS3_SA
BBAS3_SA_train = '/content/BBAS3_SA_treino.csv'
BBAS3_SA_test = '/content/BBAS3_SA_teste.csv'

# Base dados - CSNA3_SA
CSNA3_SA_train = '/content/CSNA3_SA_treino.csv'
CSNA3_SA_test = '/content/CSNA3_SA_teste.csv'

# Base dados - PETR4_SA
PETR4_SA_train = '/content/PETR4_SA_treino.csv'
PETR4_SA_test = '/content/PETR4_SA_teste.csv'

# Base dados - VALE3_SA
VALE3_SA_train = '/content/VALE3_SA_treino.csv'
VALE3_SA_test = '/content/VALE3_SA_teste.csv'

# Função auxiliar para carregar e limpar (remover Unnamed: 0)
def carrega_limpa_csv(caminho):
    return pd.read_csv(caminho, usecols=lambda c: c != 'Unnamed: 0')

## Carregando dataset
# BBAS3
df_train_bbas = carrega_limpa_csv(BBAS3_SA_train)
df_test_bbas  = carrega_limpa_csv(BBAS3_SA_test)

# CSNA3
df_train_csna = carrega_limpa_csv(CSNA3_SA_train)
df_test_csna  = carrega_limpa_csv(CSNA3_SA_test)

# PETR4
df_train_petr = carrega_limpa_csv(PETR4_SA_train)
df_test_petr  = carrega_limpa_csv(PETR4_SA_test)

# VALE3
df_train_vale = carrega_limpa_csv(VALE3_SA_train)
df_test_vale  = carrega_limpa_csv(VALE3_SA_test)

"""**Modelo 1 - Vale**"""

# Carregar dados
df_train = pd.read_csv(VALE3_SA_train)
df_test  = pd.read_csv(VALE3_SA_test)

# Visualizando dados
df_train

df_train.head()

df_train.tail()

df_train.shape

df_train.info()

df_train.dtypes

"""# **Parte 2.1 - Processamento dados**"""

df_train['Date'] = pd.to_datetime(df_train['Date'], errors='coerce')
df_train = df_train.dropna(subset=['Date'])  # remove datas inválidas
df_train

# Carregar as bases
df_bbas = pd.read_csv(BBAS3_SA_train)
df_csna = pd.read_csv(CSNA3_SA_train)
df_petr = pd.read_csv(PETR4_SA_train)
df_vale = pd.read_csv(VALE3_SA_train)

# Renomear colunas para evitar conflito, exceto Unnamed: 0
df_bbas = df_bbas.add_prefix('BBAS3_')
df_csna = df_csna.add_prefix('CSNA3_')
df_petr = df_petr.add_prefix('PETR4_')
df_vale = df_vale.add_prefix('VALE3_')

# Carregar bases
dfs = []

#
for ticker, path in [('BBAS3', BBAS3_SA_train),
                     ('CSNA3', CSNA3_SA_train),
                     ('PETR4', PETR4_SA_train),
                     ('VALE3', VALE3_SA_train)]:

    #
    df = pd.read_csv(path)

    #
    df['Ticker'] = ticker

    #
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

    #
    dfs.append(df)

# Concatenar tudo (long format)
data = pd.concat(dfs, ignore_index=True)

# (Opcional) Organizar colunas
cols = ['Ticker', 'Date'] + [col for col in data.columns if col not in ['Ticker','Date']]
data = data[cols]

# Visualizando dataset
data

"""**OBS:** **Nesse merge foi feito ajutando todos os datasets para ser usado para analise exploratoria de dados não ser aplicado para modelos de redes neurais !**

# **Parte 4 - Limpeza de dados**
"""

# 1. Conferir dados faltantes
print("Dados faltantes (treino):")
print(df_train.isnull().sum())

# 4. Conferir tipos de dados
print(df_train.dtypes)

#
df_train = df_train.drop(columns=['Unnamed: 0'])
df_test = df_test.drop(columns=['Unnamed: 0'])

#
df_train

# Remove duplicatas (caso existam)
df_train = df_train.drop_duplicates()
df_test = df_test.drop_duplicates()

# Checa se só tem labels válidos (+1 ou -1)
assert set(df_train['Label'].unique()).issubset({-1, 1}), "Atenção: Label inválido detectado!"
assert set(df_test['Label'].unique()).issubset({-1, 1}), "Atenção: Label inválido detectado!"

# (Opcional) Conferir valores negativos
for col in [c for c in df_train.columns if 'Close' in c]:
    if (df_train[col] < 0).any():
        print(f"Atenção: valores negativos em {col}")

# Ordena por data (se tiver coluna 'Data')
if 'Data' in df_train.columns:
    df_train = df_train.sort_values('Data').reset_index(drop=True)
    df_test = df_test.sort_values('Data').reset_index(drop=True)

# Pronto para o pipeline do modelo!
print("Base de dados VALE3.SA limpa e pronta.")

"""# **Parte 5 - Análise exploratoria de dados - **Empresa Vale****

**Perguntas de negocio resolvendo com analise de dados - Empresa Vale**

**Nesse parte de analise exploratoria vamos visualizar dados da empresa Vale**

**1. Qual a proporção de dias de “compra” (+1) versus “venda” (-1) no histórico?**

**2. Como evoluíram as decisões de compra e venda ao longo do tempo?**

**3. O modelo de tendência (Label) costuma acertar os movimentos reais do preço?**

**4. Qual a média e o desvio padrão das variações diárias de preço?**

**5. Há padrões sazonais ou tendências na série Smoothed_Close?**

**6. A média móvel de 5 dias antecipa mudanças de tendência nos preços?**

**7. Como a diferença entre a média móvel de 5 e 15 dias se comporta ao longo do tempo?**

**8. Quantas vezes o preço fechou acima da média móvel de 15 dias?**

**9. Quando há cruzamento de médias (MA_5 cruza MA_15 para cima ou para baixo), como o preço se comporta depois?**

**10. Como seria o retorno acumulado de uma estratégia simples “golden cross” (compra no cruzamento de alta, vende no de baixa)?**

**1. Qual a proporção de dias de “compra” (+1) versus “venda” (-1) no histórico?**
"""

#
df_plot = df_train.copy()

#
df_plot['Date'] = pd.to_datetime(df_plot['Date'])

# Proporção de labels
compra = (df_train['Label'] == 1).mean()
venda = (df_train['Label'] == -1).mean()

#
labels = ['Compra (+1)', 'Venda (-1)']
sizes = [(df_train['Label'] == 1).sum(), (df_train['Label'] == -1).sum()]
props = [f"{100*compra:.1f}%", f"{100*venda:.1f}%"]
colors = ['#53A548', '#E77D11']

plt.figure(figsize=(10, 5))
bars = plt.bar(labels, sizes, color=colors, edgecolor='black', width=0.6)

# Adiciona valor absoluto e proporção acima de cada barra
for bar, valor, prop in zip(bars, sizes, props):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(sizes)*0.01,
             f'{valor}\n({prop})', ha='center', va='bottom', fontsize=12, weight='bold')

plt.title('Quantidade e Proporção de Dias de Compra e Venda', fontsize=8, weight='bold')
plt.ylabel('Número de dias', fontsize=12)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', color='gray', alpha=0.4)

# Visual clean: remove bordas laterais
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.tight_layout()
plt.show()

print(f"Proporção de dias de compra: {compra:.2%}")
print(f"Proporção de dias de venda: {venda:.2%}")



"""**2. Como evoluíram as decisões de compra e venda ao longo do tempo?**"""

serie = df['Label'].cumsum()
datas = df['Date']
idx_max = serie.idxmax()
idx_min = serie.idxmin()

plt.figure(figsize=(30.5, 10), dpi=150)
plt.plot(datas, serie, color='#2369B2', linewidth=2.5, label='Acumulado de sinais (+1/-1)')

# Máximo (texto para baixo, cor verde)
plt.scatter(datas[idx_max], serie.max(), color='green', s=80, zorder=5, edgecolor='black')
plt.axvline(datas[idx_max], color='green', linestyle=':', linewidth=1)
plt.annotate(f'Máx: {serie.max():.0f}',
             xy=(datas[idx_max], serie.max()), xytext=(0, -70), textcoords='offset points',
             arrowprops=dict(arrowstyle='->', color='green', lw=2),
             color='green', fontsize=14, weight='bold', ha='center')

# Mínimo (texto para baixo, cor vermelha)
plt.scatter(datas[idx_min], serie.min(), color='red', s=80, zorder=5, edgecolor='black')
plt.axvline(datas[idx_min], color='red', linestyle=':', linewidth=1)
plt.annotate(f'Mín: {serie.min():.0f}',
             xy=(datas[idx_min], serie.min()), xytext=(0, -70), textcoords='offset points',
             arrowprops=dict(arrowstyle='->', color='red', lw=2),
             color='red', fontsize=14, weight='bold', ha='center')

plt.axhline(0, color='gray', linestyle='--', linewidth=1)
plt.title('Soma Acumulada dos Sinais de Compra (+1) e Venda (-1)', fontsize=20, weight='bold', pad=20)
plt.xlabel('Data', fontsize=15)
plt.ylabel('Acumulado de sinais', fontsize=15)
plt.xticks(fontsize=13)
plt.yticks(fontsize=13)
plt.grid(axis='y', linestyle='--', color='lightgray', alpha=0.5)
plt.gca().xaxis.set_major_locator(mdates.YearLocator())
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)

# LEGENDA AO LADO
plt.legend(loc='center left', bbox_to_anchor=(1.01, 0.5), fontsize=14, frameon=False)

plt.tight_layout(rect=[0, 0, 0.87, 1])  # deixa espaço à direita para legenda
plt.show()



"""**3. O modelo de tendência (Label) costuma acertar os movimentos reais do preço?**"""

# Quando label é 1, o preço subiu no próximo dia? E quando label é -1, caiu?
df = df_train.copy()
df['Next_Close'] = df['Close'].shift(-1)
df['Movimento'] = np.where(df['Next_Close'] > df['Close'], 1, -1)
df['Label_Acertou'] = (df['Label'] == df['Movimento'])

#
acertos = df['Label_Acertou'].sum()
erros = len(df) - acertos

plt.figure(figsize=(10, 8))
bars = plt.bar(['Acerto', 'Erro'], [acertos, erros], color=['#53A548', '#E77D11'])

# Valor absoluto dentro da barra
for i, bar in enumerate(bars):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2,
             f"{int(bar.get_height())}",
             ha='center', va='center', color='white', fontsize=13, weight='bold')
# Porcentagem acima
plt.text(0, acertos + max(acertos, erros)*0.02, f"{100*acertos/len(df):.1f}%",
         ha='center', color='#53A548', fontsize=13, weight='bold')
plt.text(1, erros + max(acertos, erros)*0.02, f"{100*erros/len(df):.1f}%",
         ha='center', color='#E77D11', fontsize=13, weight='bold')

plt.title('Acertos x Erros dos Sinais de Label', fontsize=16, weight='bold')
plt.ylabel('Quantidade de dias', fontsize=13)
plt.xticks(fontsize=12, weight='bold')
plt.yticks(fontsize=11)
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(False)
plt.tight_layout()
plt.show()

#
acuracia = df['Label_Acertou'].mean()
print()
print(f"Acurácia do label em prever o movimento real do próximo fechamento: {acuracia:.2%}")



"""**4. Qual a média e o desvio padrão das variações diárias de preço?**"""

#
df = df_train.copy()
df['Variação_%'] = df['Close'].pct_change() * 100
media = df['Variação_%'].mean()
desvio = df['Variação_%'].std()

#
plt.figure(figsize=(15,4), dpi=120)
plt.plot(df['Date'], df['Variação_%'], color='#2369B2', linewidth=0.8, label='Variação diária')

# Linhas estatísticas
plt.axhline(media, color='red', linestyle='--', linewidth=1.5, label=f'Média: {media:.2f}%')
plt.axhline(media+desvio, color='#008f39', linestyle=':', linewidth=1.5, label=f'+1 DP: {media+desvio:.2f}%')
plt.axhline(media-desvio, color='#008f39', linestyle=':', linewidth=1.5, label=f'-1 DP: {media-desvio:.2f}%')

# Marcar maiores outliers (opcional)
for idx in df['Variação_%'].nlargest(3).index:
    plt.plot(df.loc[idx, 'Date'], df.loc[idx, 'Variação_%'], 'o', color='purple', markersize=7)
for idx in df['Variação_%'].nsmallest(3).index:
    plt.plot(df.loc[idx, 'Date'], df.loc[idx, 'Variação_%'], 'o', color='crimson', markersize=7)

plt.title('Variação Percentual Diária (Fechamento VALE3.SA)', fontsize=15, weight='bold')
plt.xlabel('Data', fontsize=12)
plt.ylabel('Variação diária (%)', fontsize=12)
plt.grid(axis='y', linestyle='--', color='gray', alpha=0.3)
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.xticks(fontsize=11)
plt.yticks(fontsize=11)
plt.gca().xaxis.set_major_locator(mdates.YearLocator())
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))

# LEGEND INSIDE
plt.legend(loc='lower left', fontsize=11, frameon=True, facecolor='white', edgecolor='lightgray')

plt.tight_layout()
plt.show()

print(f"Variação média diária: {media:.2f}%")
print(f"Desvio padrão da variação diária: {desvio:.2f}%")

"""**5. Há padrões sazonais ou tendências na série Smoothed_Close?**"""

mask = (df_plot['Date'] >= '2015-01-01') & (df_plot['Date'] <= '2020-12-31')
df_zoom = df_plot.loc[mask]
plt.figure(figsize=(12,5))
plt.plot(df_zoom['Date'], df_zoom['Smoothed_Close'], color='#2565AE', linewidth=2)
plt.title("Evolução do Preço Suavizado (2015-2020)", fontsize=15)
plt.xlabel("Data")
plt.ylabel("Preço Suavizado (R$)")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""**6. A média móvel de 5 dias antecipa mudanças de tendência nos preços?**"""

df = df_train.copy()
df['Date'] = pd.to_datetime(df['Date'])
df['MA_5'] = df['Close'].rolling(window=5).mean()

plt.figure(figsize=(15,5))
plt.plot(df['Date'], df['Close'], color='#2369B2', linewidth=1.7, label='Preço de Fechamento')
plt.plot(df['Date'], df['MA_5'], color='#F99B24', linewidth=2.0, label='Média Móvel 5 dias')

plt.title('Preço vs. Média Móvel de 5 dias', fontsize=17, weight='bold')
plt.xlabel('Data', fontsize=13)
plt.ylabel('Preço (R$)', fontsize=13)
plt.legend(loc='upper left', fontsize=13, frameon=False)

# Formatar eixo X para anos
plt.gca().xaxis.set_major_locator(mdates.YearLocator())
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))

plt.grid(axis='y', color='lightgray', linestyle='--', linewidth=0.8, alpha=0.7)
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)

plt.tight_layout()
plt.show()



"""**7. Como a diferença entre a média móvel de 5 e 15 dias se comporta ao longo do tempo?**"""

# Calcule as médias móveis e a diferença antes de plotar
df['MA_5'] = df['Close'].rolling(window=5).mean()
df['MA_15'] = df['Close'].rolling(window=15).mean()
df['MA_diff'] = df['MA_5'] - df['MA_15']

plt.figure(figsize=(15,5))
plt.plot(df['Date'], df['MA_diff'], color='#2369B2', linewidth=1.5, label='MA_5 - MA_15')

plt.fill_between(df['Date'], df['MA_diff'], 0, where=(df['MA_diff'] > 0), color='#7FC7FF', alpha=0.25)
plt.fill_between(df['Date'], df['MA_diff'], 0, where=(df['MA_diff'] < 0), color='#FF8C7F', alpha=0.25)

plt.axhline(0, color='red', linestyle='--', linewidth=1)
plt.title('Diferença entre Média Móvel de 5 e 15 dias', fontsize=16, weight='bold')
plt.xlabel('Data', fontsize=13)
plt.ylabel('Diferença (R$)', fontsize=13)
plt.legend(loc='upper left', fontsize=13, frameon=False)

plt.gca().xaxis.set_major_locator(mdates.YearLocator())
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))

plt.grid(axis='y', color='lightgray', linestyle='--', linewidth=0.8, alpha=0.7)
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.tight_layout()
plt.show()

"""**8. Quantas vezes o preço fechou acima da média móvel de 15 dias?**"""

acima_ma15 = (df['Close'] > df['MA_15']).sum()
abaixo_ma15 = (df['Close'] <= df['MA_15']).sum()

plt.figure(figsize=(10, 8))
plt.bar(['Acima da MA_15', 'Abaixo da MA_15'], [acima_ma15, abaixo_ma15], color=['#2369B2','#FF8C7F'])
plt.title('Dias em que o preço fechou acima ou abaixo da MA_15')
plt.ylabel('Número de dias')
plt.show()

acima_ma15 = (df['Close'] > df['MA_15']).sum()
total = df['Close'].count()
print(f"O preço fechou acima da MA_15 em {acima_ma15} de {total} dias ({100*acima_ma15/total:.1f}%).")

"""**9. Quando há cruzamento de médias (MA_5 cruza MA_15 para cima ou para baixo), como o preço se comporta depois?**"""

# Cruzamento de alta: MA_5 cruza MA_15 de baixo para cima
df['MA_5_prev'] = df['MA_5'].shift(1)
df['MA_15_prev'] = df['MA_15'].shift(1)
df['cruzou_cima'] = ((df['MA_5_prev'] < df['MA_15_prev']) & (df['MA_5'] >= df['MA_15']))
df['cruzou_baixo'] = ((df['MA_5_prev'] > df['MA_15_prev']) & (df['MA_5'] <= df['MA_15']))

plt.figure(figsize=(20.5, 10))
plt.plot(df['Date'], df['Close'], color='#2369B2', label='Preço de Fechamento', linewidth=1)
plt.plot(df['Date'], df['MA_5'], color='#F99B24', label='MA 5 dias', linewidth=1.2)
plt.plot(df['Date'], df['MA_15'], color='#53A548', label='MA 15 dias', linewidth=1.2)

plt.scatter(df.loc[df['cruzou_cima'], 'Date'], df.loc[df['cruzou_cima'], 'Close'],
            marker='^', color='green', s=60, label='Cruzamento de Alta (MA5↑MA15)')
plt.scatter(df.loc[df['cruzou_baixo'], 'Date'], df.loc[df['cruzou_baixo'], 'Close'],
            marker='v', color='red', s=60, label='Cruzamento de Baixa (MA5↓MA15)')

plt.title('Cruzamentos de Médias Móveis (5x15 dias) sobre Preço - VALE', fontsize=15, weight='bold')
plt.xlabel('Data')
plt.ylabel('Preço (R$)')

# LEGENDA NA LATERAL
plt.legend(fontsize=10, loc='center left', bbox_to_anchor=(1.01, 0.5), borderaxespad=0)

plt.gca().xaxis.set_major_locator(mdates.YearLocator())
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.tight_layout(rect=[0, 0, 0.82, 1])  # Deixa espaço para legenda ao lado
plt.show()

print(f"Nº de cruzamentos de alta (MA_5 cruzou MA_15 para cima): {df['cruzou_cima'].sum()}")
print(f"Nº de cruzamentos de baixa (MA_5 cruzou MA_15 para baixo): {df['cruzou_baixo'].sum()}")



"""**10. Como seria o retorno acumulado de uma estratégia simples “golden cross” (compra no cruzamento de alta, vende no de baixa)?**"""

df['sinal'] = 0
df.loc[df['cruzou_cima'], 'sinal'] = 1   # compra
df.loc[df['cruzou_baixo'], 'sinal'] = -1 # venda
df['sinal'] = df['sinal'].replace(to_replace=0, method='ffill')  # mantém posição

# Retorno diário
df['retorno'] = df['Close'].pct_change()
# Retorno da estratégia (multiplica pelo sinal)
df['retorno_estrategia'] = df['retorno'] * df['sinal']
retorno_acumulado = (1 + df['retorno_estrategia'].fillna(0)).cumprod() - 1
retorno_buy_hold = (1 + df['retorno'].fillna(0)).cumprod() - 1


# Plot
fig, axs = plt.subplots(2, 1, figsize=(15,10), sharex=True)

# --- Gráfico 1: Preço e Médias Móveis ---
axs[0].plot(df['Date'], df['Close'], color='#2369B2', linewidth=1, label='Preço de Fechamento')
axs[0].plot(df['Date'], df['MA_5'], color='#F99B24', linewidth=1.5, label='MA 5 dias')
axs[0].plot(df['Date'], df['MA_15'], color='#53A548', linewidth=1.5, label='MA 15 dias')
axs[0].set_title('Preço + Médias Móveis (5 e 15 dias)', fontsize=16, weight='bold')
axs[0].set_ylabel('Preço (R$)', fontsize=13)
axs[0].legend(loc='upper left', fontsize=12, frameon=False)
axs[0].grid(axis='y', linestyle='--', color='lightgray', alpha=0.8)
axs[0].spines['top'].set_visible(False)
axs[0].spines['right'].set_visible(False)

# --- Gráfico 2: Estratégia vs Buy & Hold ---
axs[1].plot(df['Date'], retorno_acumulado, color='#2369B2', linewidth=2.2, label='Estratégia Cruzamento MA')
axs[1].plot(df['Date'], retorno_buy_hold, color='#E77D11', linestyle='--', linewidth=1.7, label='Buy & Hold')
axs[1].axhline(0, color='gray', linestyle='--', linewidth=1)
axs[1].set_title('Backtest: Estratégia Cruzamento MA x Buy & Hold', fontsize=16, weight='bold')
axs[1].set_xlabel('Data', fontsize=13)
axs[1].set_ylabel('Retorno acumulado (%)', fontsize=13)
axs[1].legend(loc='upper left', fontsize=12, frameon=False)
axs[1].grid(axis='y', linestyle='--', color='lightgray', alpha=0.8)
axs[1].spines['top'].set_visible(False)
axs[1].spines['right'].set_visible(False)

# Ajuste eixo X para datas (anos)
axs[1].xaxis.set_major_locator(mdates.YearLocator())
axs[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))

plt.tight_layout()
plt.show()

"""# **Part 5.1 - Analise exploratoria de dados - Empresa todas**

**Perguntas de negocio resolvendo com Análise de dados**

1. **Como evoluiu o preço de fechamento de cada ação ao longo do tempo?**

   → **Identifica tendências, crises e períodos de alta.**

2. **Qual a proporção de sinais de compra (+1) e venda (-1) para cada ação?**

   → **Avalia o balanço dos sinais emitidos por ativo.**

3. **A média móvel de 5 dias (MA\_5) é um bom indicador para os sinais de compra/venda?**

   → **Proporção de dias em que Label=1 e Close > MA\_5, e vice-versa.**

4. **Quando o preço cruza para cima da média móvel de 15 dias (MA\_15), há maior frequência de sinais de compra?**

   → **Relaciona cruzamentos com a coluna Label.**

5. **Existe diferença significativa na volatilidade diária (Close) entre as ações?**

   → **Usa desvio padrão/boxplot de variação percentual diária.**

6. **A estratégia de cruzamento de médias móveis (MA\_5 e MA\_15) teria gerado mais sinais de compra ou de venda?**

   → **Quantifica cruzamentos de alta e baixa.**

7. **Qual ação teve maior número de cruzamentos de médias móveis nos últimos 10 anos?**

   → **Mede a frequência de cruzamentos, um sinal de maior volatilidade.**

8. **Qual o retorno acumulado de uma estratégia simples: comprar quando Close > MA\_5 e vender quando Close < MA\_5?**

   → **Simula um backtest e compara com buy & hold.**

9. **Em quantos dias o preço fechou acima de ambas as médias móveis (MA\_5 e MA\_15) para cada ação?**

   → **Indica possíveis tendências de força.**

10. **Os sinais de compra/venda (Label) ocorrem mais frequentemente próximos a máximas ou mínimas móveis de 15 dias?**

    → **Relaciona Label com extremos móveis.**

**Qual a proporção de sinais de compra e venda (Label) para cada ativo?**

- **Ver se algum ativo tem tendência maior para compra ou venda ao longo do tempo.**
"""

data.groupby('Ticker')['Label'].value_counts(normalize=True).unstack().plot(kind='bar', stacked=True)
plt.title('Proporção de sinais de compra e venda por ativo')
plt.ylabel('Proporção')
plt.show()

"""**O sinal de Label (+1/-1) realmente antecipa o movimento do preço no dia seguinte?**

- **Validar o poder preditivo do rótulo fornecido.**
"""

data['Next_Close'] = data.groupby('Ticker')['Close'].shift(-1)
data['Movimento'] = np.where(data['Next_Close'] > data['Close'], 1, -1)
acuracia = (data['Label'] == data['Movimento']).mean()
print(f'Acurácia do label em prever o movimento do próximo fechamento: {acuracia:.2%}')

"""**Como se comportam as variações percentuais diárias dos ativos? (Volatilidade)**

- **Qual ação é mais volátil? Qual tem dias de variação extrema ?**
"""

data['Variação_%'] = data.groupby('Ticker')['Close'].pct_change() * 100
sns.boxplot(data=data, x='Ticker', y='Variação_%')
plt.title('Distribuição da variação percentual diária por ativo')
plt.ylabel('Variação diária (%)')
plt.show()

"""**Questão 1)** Como evoluiu o preço de fechamento de cada ação ao longo do tempo?

- **Identifica tendências, crises e períodos de alta.**
"""

plt.figure(figsize=(14,6))

# Paleta customizada para os tickers
cores = {'BBAS3': '#1f77b4', 'CSNA3': '#ff7f0e', 'PETR4': '#2ca02c', 'VALE3': '#d62728'}

# Plot customizado com Seaborn
sns.lineplot(data=data, x='Date', y='Close', hue='Ticker', palette=cores, linewidth=2.2)

plt.title('Evolução do Preço de Fechamento por Ação (2000–2019)', fontsize=18, weight='bold')
plt.ylabel('Preço de Fechamento (R$)', fontsize=15)
plt.xlabel('Data', fontsize=14)
plt.legend(title='Ticker', title_fontsize=13, fontsize=12, loc='center left', bbox_to_anchor=(1.01, 0.5), frameon=False)
plt.grid(axis='y', linestyle='--', alpha=0.4)

# Deixar ticks de ano em ano
plt.gca().xaxis.set_major_locator(mdates.YearLocator())
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
plt.xticks(rotation=45)
plt.tight_layout()

# Remove as bordas superiores e à direita
sns.despine()

plt.show()

"""**Questão 2)** Qual a proporção de sinais de compra (+1) e venda (-1) para cada ação?

- **Avalia o balanço dos sinais emitidos por ativo.**
"""

# Redefine a ordem para garantir: Venda (-1) embaixo, Compra (+1) em cima
df_prop = data.groupby('Ticker')['Label'].value_counts(normalize=True).unstack()[[-1,1]]

fig, ax = plt.subplots(figsize=(8,5))
bars = df_prop.plot(kind='bar', stacked=True, color=['#E77D11', '#53A548'], ax=ax, width=0.7, edgecolor='k')

# Adiciona porcentagens nas barras
for i, ticker in enumerate(df_prop.index):
    v_pct = df_prop.loc[ticker, -1]
    c_pct = df_prop.loc[ticker, 1]
    ax.text(i, v_pct/2, f"{v_pct:.1%}", ha='center', va='center', color='white', fontsize=13, fontweight='bold')
    ax.text(i, v_pct + c_pct/2, f"{c_pct:.1%}", ha='center', va='center', color='white', fontsize=13, fontweight='bold')

plt.title('Proporção de Sinais de Compra (+1) e Venda (-1) por Ação', fontsize=15, weight='bold')
plt.ylabel('Proporção', fontsize=12)
plt.xlabel('Ação', fontsize=12)
plt.xticks(rotation=0, fontsize=11)
plt.yticks(fontsize=11)
plt.grid(axis='y', linestyle='--', alpha=0.3)

# Legenda fora do gráfico, sem borda
plt.legend(['Venda (-1)', 'Compra (+1)'], loc='center left', bbox_to_anchor=(1.01, 0.5), frameon=False, fontsize=12)
plt.tight_layout()
plt.show()

"""**Questão 3)** A média móvel de 5 dias (MA_5) é um bom indicador para os sinais de compra/venda?

- Proporção de dias em que Label=1 e Close > MA_5, e vice-versa.
"""

data['MA_5'] = data.groupby('Ticker')['Close'].transform(lambda x: x.rolling(5).mean())
compra_ma5 = ((data['Label'] == 1) & (data['Close'] > data['MA_5'])).mean()
venda_ma5  = ((data['Label'] == -1) & (data['Close'] < data['MA_5'])).mean()

# Proporções já calculadas
props = [compra_ma5, venda_ma5]
labels = [
    'Compra (+1) com Close > MA_5',
    'Venda (-1) com Close < MA_5'
]
colors = ['#53A548', '#E77D11']

plt.figure(figsize=(7,5))
bars = plt.bar(labels, props, color=colors, edgecolor='k', width=0.6)

# Adiciona valores percentuais nas barras
for bar, value in zip(bars, props):
    plt.text(bar.get_x() + bar.get_width()/2, value + 0.01, f"{value:.1%}", ha='center', va='bottom', fontsize=14, fontweight='bold')

plt.title('Alinhamento dos Sinais de Compra/Venda com MA_5', fontsize=15, weight='bold')
plt.ylabel('Proporção dos Dias', fontsize=12)
plt.ylim(0, 1)
plt.xticks(fontsize=12, rotation=10)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.3)
plt.tight_layout()
plt.show()

print(f"Proporção de dias Label=1 e Close > MA_5: {compra_ma5:.2%}")
print(f"Proporção de dias Label=-1 e Close < MA_5: {venda_ma5:.2%}")

"""**Questão 4)** Quando o preço cruza para cima da média móvel de 15 dias (MA_15), há maior frequência de sinais de compra ?

- Relaciona cruzamentos com a coluna Label.
"""

# 4. Cruzamento de alta com MA_15 e frequência de Label=1
data['MA_15'] = data.groupby('Ticker')['Close'].transform(lambda x: x.rolling(15).mean())
data['MA_5_prev'] = data.groupby('Ticker')['MA_5'].shift(1)
data['MA_15_prev'] = data.groupby('Ticker')['MA_15'].shift(1)
data['cruzou_cima'] = ((data['MA_5_prev'] < data['MA_15_prev']) & (data['MA_5'] >= data['MA_15']))
cruzamentos_compra = data[data['cruzou_cima']]['Label'].value_counts()

# Dados dos cruzamentos
cruzamentos = cruzamentos_compra.sort_index()  # Garante ordem -1, 1
labels = ['Venda (-1)', 'Compra (+1)']
colors = ['#E77D11', '#53A548']

plt.figure(figsize=(10, 5))
bars = plt.bar(labels, cruzamentos.values, color=colors, edgecolor='k', width=0.5)

# Adiciona os valores absolutos nas barras
for bar, value in zip(bars, cruzamentos.values):
    plt.text(bar.get_x() + bar.get_width()/2, value + 5, str(value), ha='center', va='bottom', fontsize=13, fontweight='bold')

plt.title('Frequência de Sinais após Cruzamento de Alta (MA_5 ↑ MA_15)', fontsize=14, weight='bold')
plt.ylabel('Número de Ocorrências', fontsize=12)
plt.ylim(0, max(cruzamentos.values)*1.15)
plt.xticks(fontsize=12)
plt.yticks(fontsize=11)
plt.tight_layout()
plt.show()

print("\nFrequência de Label após cruzamento de alta (MA_5 cruzou MA_15 para cima):")
print(cruzamentos_compra)

"""**Questão 5)** Existe diferença significativa na volatilidade diária (Close) entre as ações?

- Usa desvio padrão/boxplot de variação percentual diária.
"""

# Diferença de volatilidade entre ações (boxplot variação % diária)
plt.figure(figsize=(10,6))
sns.set_style('whitegrid')

# Ordem dos tickers pela mediana da volatilidade
ordem = (
    data.groupby('Ticker')['Variação_%']
    .median()
    .sort_values(ascending=False)
    .index
)

# Plot principal
ax = sns.boxplot(
    data=data,
    x='Ticker',
    y='Variação_%',
    order=ordem,
    palette='pastel',
    showfliers=False,
    linewidth=2
)

# Mediana e média anotadas
for i, ticker in enumerate(ordem):
    mediana = data.loc[data['Ticker']==ticker, 'Variação_%'].median()
    media   = data.loc[data['Ticker']==ticker, 'Variação_%'].mean()
    plt.scatter(i, mediana, color='blue', marker='o', s=70, zorder=10, label='Mediana' if i==0 else "")
    plt.scatter(i, media, color='red', marker='X', s=70, zorder=10, label='Média' if i==0 else "")
    plt.text(i+0.05, mediana, f"{mediana:.2f}", color='blue', fontsize=10, va='center', fontweight='bold')
    plt.text(i+0.05, media, f"{media:.2f}", color='red', fontsize=10, va='center')

# Ajustes estéticos
plt.title('Volatilidade Diária (% de Variação) por Ação', fontsize=16, weight='bold')
plt.ylabel('Variação diária (%)', fontsize=13)
plt.xlabel('Ticker', fontsize=13)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.xticks(fontsize=12)
plt.yticks(fontsize=11)
plt.legend(loc='upper right', fontsize=11, frameon=False)
plt.tight_layout()

# Nota de rodapé (opcional)
n_obs = data['Ticker'].value_counts().to_dict()
nota = " | ".join([f"{tkr}: {n} dias" for tkr, n in n_obs.items()])
plt.figtext(0.5, -0.05, f"Número de observações por ativo: {nota}", ha='center', fontsize=10, color='gray')

plt.show()

"""**Questão 6)** A estratégia de cruzamento de médias móveis (MA_5 e MA_15) teria gerado mais sinais de compra ou de venda ?

- Quantifica cruzamentos de alta e baixa.
"""

# 6. Quantidade de cruzamentos de alta/baixa (MA_5 e MA_15)
data['cruzou_baixo'] = ((data['MA_5_prev'] > data['MA_15_prev']) & (data['MA_5'] <= data['MA_15']))

# Se ainda não fez, garanta que está agrupando por ticker!
cruzamentos = (
    data.groupby('Ticker')[['cruzou_cima', 'cruzou_baixo']]
    .sum()
    .rename(columns={'cruzou_cima': 'Alta (MA_5↑MA_15)', 'cruzou_baixo': 'Baixa (MA_5↓MA_15)'})
    .astype(int)
)
print(cruzamentos)

print("\nCruzamentos de alta (MA_5↑MA_15):", data['cruzou_cima'].sum())
print("Cruzamentos de baixa (MA_5↓MA_15):", data['cruzou_baixo'].sum())

# Tamanho maior, ex: (12, 7) ou (14, 7)
fig, ax = plt.subplots(figsize=(12, 7))  # Altere para o tamanho desejado

# Plotando novamente (use o mesmo DataFrame cruzamentos)
cruzamentos.plot(
    kind='bar',
    color=['#53A548', '#E77D11'],
    width=0.8,
    ax=ax
)

plt.title('Quantidade de Cruzamentos de Alta e Baixa por Ação (MA_5 x MA_15)', fontsize=9, weight='bold')
plt.xlabel('Ticker', fontsize=15)
plt.ylabel('Nº de Cruzamentos', fontsize=15)
plt.xticks(rotation=0, fontsize=13)
plt.yticks(fontsize=13)
plt.grid(axis='y', linestyle='--', alpha=0.3)

# Legenda fora do gráfico
plt.legend(
    title='Tipo de Cruzamento',
    bbox_to_anchor=(1.02, 1),
    loc='upper left',
    borderaxespad=0,
    frameon=False,
    fontsize=13,
    title_fontsize=15
)

plt.tight_layout(rect=[0, 0, 0.87, 1])  # Ajuste para não cortar a legenda
plt.show()

"""**Questão 7)** Qual ação teve maior número de cruzamentos de médias móveis nos últimos 10 anos?

- Mede a frequência de cruzamentos, um sinal de maior volatilidade.
"""

# Define o ano máximo e filtra os últimos 10 anos
ultimo_ano = data['Date'].max().year
df10 = data[data['Date'] >= f'{ultimo_ano-9}-01-01']

# Calcula os cruzamentos de alta (MA_5 cruzou MA_15 para cima) por ativo
cruzamentos_por_ativo = df10.groupby('Ticker')['cruzou_cima'].sum()

# Ordena para visualização
cruzamentos_plot = cruzamentos_por_ativo.sort_values(ascending=False)

plt.figure(figsize=(20.5, 10))
bars = plt.bar(cruzamentos_plot.index, cruzamentos_plot.values, color='#2369B2')

plt.title('Cruzamentos de Alta (MA_5↑MA_15) nos Últimos 10 Anos por Ativo', fontsize=16, weight='bold')
plt.ylabel('Nº de Cruzamentos de Alta')
plt.xlabel('Ticker')

# Índice da maior barra
max_idx = cruzamentos_plot.values.argmax()

# Adiciona valor acima das barras (menos na maior)
for i, valor in enumerate(cruzamentos_plot.values):
    if i != max_idx:
        plt.text(
            cruzamentos_plot.index[i],
            valor + 1.5,
            f"{valor}",
            ha='center',
            va='bottom',
            fontsize=13,
            fontweight='bold',
            color='#444'
        )

# Destaca a maior barra e adiciona valor
bars[max_idx].set_color('#F99B24')
plt.text(
    cruzamentos_plot.index[max_idx],
    cruzamentos_plot.values[max_idx] + 4,
    f"{cruzamentos_plot.values[max_idx]}",
    color='#F99B24',
    fontsize=15,
    fontweight='bold',
    ha='center'
)

plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

print("\nCruzamentos de alta nos últimos 10 anos por ativo:")
print(cruzamentos_por_ativo)
print("Ativo com mais cruzamentos:", cruzamentos_por_ativo.idxmax())

"""**Questão 8)** Qual o retorno acumulado de uma estratégia simples: comprar quando Close > MA_5 e vender quando Close < MA_5?

- Simula um backtest e compara com buy & hold.
"""

# 8. Retorno acumulado da estratégia (Close > MA_5 = compra, <MA_5 = venda)
data['sinal'] = np.where(data['Close'] > data['MA_5'], 1, -1)
data['retorno'] = data.groupby('Ticker')['Close'].pct_change()
data['retorno_estrategia'] = data['retorno'] * data['sinal'].shift(1)

for ticker in data['Ticker'].unique():
    df_t = data[data['Ticker'] == ticker].copy()
    ret = (1 + df_t['retorno_estrategia'].fillna(0)).cumprod() - 1
    ret_bh = (1 + df_t['retorno'].fillna(0)).cumprod() - 1

    plt.figure(figsize=(14,5))
    plt.plot(df_t['Date'], 100*ret, label='Estratégia MA_5', linewidth=2, color='#2369B2')
    plt.plot(df_t['Date'], 100*ret_bh, label='Buy & Hold', linestyle='--', linewidth=2, color='#F99B24')

    plt.axhline(0, color='gray', linestyle=':', linewidth=1)
    plt.title(f'Retorno Acumulado (%) - {ticker}', fontsize=15, weight='bold')
    plt.xlabel('Data', fontsize=13)
    plt.ylabel('Retorno acumulado (%)', fontsize=13)
    plt.legend(fontsize=12, frameon=False, loc='upper left')

    # Mostra o valor final do retorno no gráfico
    plt.text(df_t['Date'].iloc[-1], 100*ret.iloc[-1], f"{100*ret.iloc[-1]:.1f}%", color='#2369B2', fontsize=12, va='center', ha='right', weight='bold')
    plt.text(df_t['Date'].iloc[-1], 100*ret_bh.iloc[-1], f"{100*ret_bh.iloc[-1]:.1f}%", color='#F99B24', fontsize=12, va='center', ha='right', weight='bold')

    # Datas: ticks por ano
    plt.gca().xaxis.set_major_locator(mdates.YearLocator())
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))

    plt.grid(axis='y', linestyle='--', alpha=0.4)
    plt.tight_layout()
    plt.show()

"""**Questão 9)** Em quantos dias o preço fechou acima de ambas as médias móveis (MA_5 e MA_15) para cada ação?

- Indica possíveis tendências de força.
"""

# 9. Dias com preço acima de MA_5 e MA_15 (por ativo)
acima_ma = data[(data['Close'] > data['MA_5']) & (data['Close'] > data['MA_15'])]
acima_ma_count = acima_ma.groupby('Ticker').size()

# Ordenar para visualização mais clara
acima_ma_count = acima_ma_count.sort_values(ascending=False)

plt.figure(figsize=(20.5, 10))
bars = plt.bar(acima_ma_count.index, acima_ma_count.values, color='#2369B2')

# Destaca o maior valor
max_idx = acima_ma_count.values.argmax()
bars[max_idx].set_color('#F99B24')

# Rótulo acima de cada barra
for i, valor in enumerate(acima_ma_count.values):
    plt.text(
        acima_ma_count.index[i],
        valor + 25,
        f"{valor:,}",
        ha='center',
        va='bottom',
        fontsize=13,
        fontweight='bold',
        color='#444' if i != max_idx else '#F99B24'
    )

plt.title('Dias com Preço Acima de MA_5 e MA_15 por Ação', fontsize=15, weight='bold')
plt.ylabel('Nº de Dias')
plt.xlabel('Ticker')
plt.grid(axis='y', linestyle='--', alpha=0.3)
plt.tight_layout()
plt.show()

print('\nDias com preço acima das duas médias móveis:')
print(acima_ma_count)

"""**Questão 10)** Os sinais de compra/venda (Label) ocorrem mais frequentemente próximos a máximas ou mínimas móveis de 15 dias?

- Relaciona Label com extremos móveis.
"""

# 10. Sinais próximos de máximas/mínimas móveis de 15 dias
data['max_15'] = data.groupby('Ticker')['Close'].transform(lambda x: x.rolling(15).max())
data['min_15'] = data.groupby('Ticker')['Close'].transform(lambda x: x.rolling(15).min())
prox_max = ((data['Label'] == 1) & (np.isclose(data['Close'], data['max_15'], atol=0.01))).sum()
prox_min = ((data['Label'] == -1) & (np.isclose(data['Close'], data['min_15'], atol=0.01))).sum()

# Valores dos sinais
valores = [prox_max, prox_min]
labels = ['Compra próxima à máxima móvel (MAx_15)', 'Venda próxima à mínima móvel (MIn_15)']
colors = ['#53A548', '#E77D11']

plt.figure(figsize=(9, 3.8))
bars = plt.barh(labels, valores, color=colors)

# Adiciona o valor ao lado da barra
for i, v in enumerate(valores):
    plt.text(
        v + 30, i,
        f"{v:,}",
        va='center',
        fontweight='bold',
        fontsize=13,
        color=colors[i]
    )

plt.title('Sinais Próximos de Máximas/Mínimas Móveis de 15 Dias', fontsize=15, weight='bold')
plt.xlabel('Quantidade de Sinais')
plt.xlim(0, max(valores) + 300)
plt.grid(axis='x', linestyle='--', alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nSinais de compra próximos da máxima móvel de 15 dias: {prox_max}")
print(f"Sinais de venda próximos da mínima móvel de 15 dias: {prox_min}")

"""# **Parte 6 - Feature engineering**"""

#

#
bbas_train = pd.read_csv(BBAS3_SA_train, usecols=lambda c: c != 'Unnamed: 0')
bbas_test  = pd.read_csv(BBAS3_SA_test,  usecols=lambda c: c != 'Unnamed: 0')

#
csna_train = pd.read_csv(CSNA3_SA_train, usecols=lambda c: c != 'Unnamed: 0')
csna_test  = pd.read_csv(CSNA3_SA_test,  usecols=lambda c: c != 'Unnamed: 0')

#
petr_train = pd.read_csv(PETR4_SA_train, usecols=lambda c: c != 'Unnamed: 0')
petr_test  = pd.read_csv(PETR4_SA_test,  usecols=lambda c: c != 'Unnamed: 0')

#
vale_train = pd.read_csv(VALE3_SA_train, usecols=lambda c: c != 'Unnamed: 0')
vale_test  = pd.read_csv(VALE3_SA_test,  usecols=lambda c: c != 'Unnamed: 0')

# Remova colunas 'Date' e 'Ticker' (e qualquer outra categórica/textual)
df_train = df_train.drop(columns=['Date', 'Ticker'], errors='ignore')
df_train  = df_train.drop(columns=['Date', 'Ticker'], errors='ignore')

# Remover colunas 'Date' e 'Ticker' de treino e teste para BBAS3
csna_train = csna_train.drop(columns=['Date', 'Ticker'], errors='ignore')
csna_test  = csna_test.drop(columns=['Date', 'Ticker'], errors='ignore')

petr_train = petr_train.drop(columns=['Date', 'Ticker'], errors='ignore')
petr_test  = petr_test.drop(columns=['Date', 'Ticker'], errors='ignore')

bbas_train = bbas_train.drop(columns=['Date', 'Ticker'], errors='ignore')
bbas_test  = bbas_test.drop(columns=['Date', 'Ticker'], errors='ignore')

# Visualizando tipos dados
print(bbas_train.dtypes)

# Cópia dataset para modelo rede neural - Recorrente
data_train = data.copy()

# Salvando dataset
data_train.to_csv("Dataset_ações_padraão.csv")

# Dataset para modelo rede neural Classficação - Rede neural Binaria (MLP)
data.head()

# Exluindo colunas nao ter caso de vazamento dados()
colunas_para_excluir = ['Date', 'Unnamed: 0', 'Next_Close', 'Movimento', 'Variação_%', 'MA_5', 'MA_15',
                        'MA_5_prev', 'MA_15_prev', 'cruzou_cima', 'cruzou_baixo', 'sinal',
                        'retorno', 'retorno_estrategia', 'max_15', 'min_15', 'ret_1d', 'ret_5d',
                        'vol_5d', 'Close_MA5_ratio', 'cruzamento_MA']

#
data = data.drop(columns=colunas_para_excluir, errors='ignore')

#
data

from sklearn.preprocessing import LabelEncoder

# Codifica ticker numericamente
le = LabelEncoder()
data['Ticker'] = le.fit_transform(data['Ticker'])

# Visualziando
le

# Visualziando
data

from sklearn.preprocessing import StandardScaler

# ========== VALE3 ==========
df_vale = data[data['Ticker'] == le.transform(['VALE3'])[0]].copy()

# ========== PETR4 ==========
df_petr = data[data['Ticker'] == le.transform(['PETR4'])[0]].copy()

# ========== BBAS3 ==========
df_bbas = data[data['Ticker'] == le.transform(['BBAS3'])[0]].copy()

# ========== CSNA3 ==========
df_csna = data[data['Ticker'] == le.transform(['CSNA3'])[0]].copy()

"""# Parte 6 - Divisão treino e teste

- Nesta etapa será realizada a divisão dos dados em conjuntos de treino e teste, etapa fundamental para a construção de modelos de redes neurais voltados para tarefas de classificação.


"""

# ==========> VALE3
X_vale = df_vale.drop(columns=['Label', 'Date', 'Ticker'], errors='ignore')
y_vale = (df_vale['Label'] == 1).astype(int)

# ==========> PETR4
X_petr = df_petr.drop(columns=['Label', 'Date', 'Ticker'], errors='ignore')
y_petr = (df_petr['Label'] == 1).astype(int)

# ==========> BBAS3
X_bbas = df_bbas.drop(columns=['Label', 'Date', 'Ticker'], errors='ignore')
y_bbas = (df_bbas['Label'] == 1).astype(int)

# ==========> CSNA3
X_csna = df_csna.drop(columns=['Label', 'Date', 'Ticker'], errors='ignore')
y_csna = (df_csna['Label'] == 1).astype(int)

# Visualizando linhas e y
print("Linhas e colunas x - VALE", X_vale.shape)
print("Linhas e colunas y - VALE", y_vale.shape)

"""# **Parte 7 - Treinamento modelo**"""

from sklearn.model_selection import train_test_split

#
X_vale_train, X_vale_test, y_vale_train, y_vale_test = train_test_split(X_vale, y_vale, test_size=0.2, stratify=y_vale, random_state=42)

#
X_petr_train, X_petr_test, y_petr_train, y_petr_test = train_test_split(X_petr, y_petr, test_size=0.2, stratify=y_petr, random_state=42)

#
X_bbas_train, X_bbas_test, y_bbas_train, y_bbas_test = train_test_split(X_bbas, y_bbas, test_size=0.2, stratify=y_bbas, random_state=42)

#
X_csna_train, X_csna_test, y_csna_train, y_csna_test = train_test_split(X_csna, y_csna, test_size=0.2, stratify=y_csna, random_state=42)


# Imprimir shapes para verificar
print('Train:', X_vale_train.shape, y_vale_train.shape)
print('Test :', X_vale_test.shape, y_vale_test.shape)

"""# Parte 8 - Normalização dados"""

from sklearn.preprocessing import StandardScaler

# Padroniza features - VALE
scaler_vale = StandardScaler()
X_vale_train_scaled = scaler_vale.fit_transform(X_vale_train)
X_vale_test_scaled = scaler_vale.transform(X_vale_test)

#
scaler_petr = StandardScaler()
X_petr_train_scaled = scaler_petr.fit_transform(X_petr_train)
X_petr_test_scaled = scaler_petr.transform(X_petr_test)

#
scaler_bbas = StandardScaler()
X_bbas_train_scaled = scaler_bbas.fit_transform(X_bbas_train)
X_bbas_test_scaled = scaler_bbas.transform(X_bbas_test)

#
scaler_csna = StandardScaler()
X_csna_train_scaled = scaler_csna.fit_transform(X_csna_train)
X_csna_test_scaled = scaler_csna.transform(X_csna_test)

#
scaler_vale

"""# **Part 8 - Rede Neural 1 - Classficação**"""

from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Dicionários
models = {}
scalers = {}
histories = {}

#
datasets = {
    'VALE3': (X_vale_train, y_vale_train, X_vale_test, y_vale_test, X_vale, y_vale),
    'PETR4': (X_petr_train, y_petr_train, X_petr_test, y_petr_test, X_petr, y_petr),
    'BBAS3': (X_bbas_train, y_bbas_train, X_bbas_test, y_bbas_test, X_bbas, y_bbas),
    'CSNA3': (X_csna_train, y_csna_train, X_csna_test, y_csna_test, X_csna, y_csna),
}


# Loop de treino
for ticker, (X_train, y_train, X_test, y_test, X_full, y_full) in datasets.items():
    print()
    print(f"Treinando modelo para {ticker}")
    print()

    #
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    #
    model = Sequential([Dense(32, activation='relu', input_shape=(X_train.shape[1],)),
                        Dropout(0.3),
                        Dense(16, activation='relu'),
                        Dropout(0.3),
                        Dense(1, activation='sigmoid')])

    #
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    #
    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    #
    history = model.fit(X_train_scaled, y_train,
                        validation_data=(X_test_scaled, y_test),
                        epochs=100,
                        batch_size=64,
                        callbacks=[early_stop],
                        verbose=1)

    #
    models[ticker] = model
    scalers[ticker] = scaler
    histories[ticker] = history

#
for ticker in histories:
    history = histories[ticker].history
    epochs = range(1, len(history['accuracy']) + 1)

    plt.figure(figsize=(14,5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, history['accuracy'], label='Treino')
    plt.plot(epochs, history['val_accuracy'], label='Validação')
    plt.title(f"Acurácia - {ticker}")
    plt.xlabel("Épocas")
    plt.ylabel("Acurácia")
    plt.legend()
    plt.grid(False)

    plt.subplot(1, 2, 2)
    plt.plot(epochs, history['loss'], label='Treino')
    plt.plot(epochs, history['val_loss'], label='Validação')
    plt.title(f"Loss - {ticker}")
    plt.xlabel("Épocas")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(False)

    plt.suptitle(f"Desempenho da Rede Neural - {ticker}", fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(f'history{ticker}.png')
    plt.show()

"""# **Parte 9 - Métricas e avaliaçãoes**"""

from sklearn.metrics import confusion_matrix

# Dados full por ticker
dados_full = {'VALE3': (X_vale, y_vale),
              'PETR4': (X_petr, y_petr),
              'BBAS3': (X_bbas, y_bbas),
              'CSNA3': (X_csna, y_csna)}

for ticker in dados_full:
    print(f"\n Matriz de Confusão - {ticker}")

    X, y = dados_full[ticker]
    X_scaled = scalers[ticker].transform(X)
    y_pred = (models[ticker].predict(X_scaled) >= 0.5).astype(int).flatten()

    cm = confusion_matrix(y, y_pred)
    sns.heatmap(cm, annot=True, cmap="Blues", fmt='d',
                xticklabels=["Venda", "Compra"],
                yticklabels=["Venda", "Compra"])
    plt.title(f"Matriz de Confusão - {ticker}")
    plt.xlabel("Predito")
    plt.ylabel("Real")
    plt.savefig(f'Matriz{ticker}.png')
    plt.show()

from sklearn.metrics import classification_report

for ticker, (X_train, y_train, X_test, y_test, X_full, y_full) in datasets.items():
    print()
    print(f"\n Classification Report - {ticker}")

    # Aplica o scaler nos dados completos
    X_scaled = scalers[ticker].transform(X_full)

    # Faz a predição com o modelo treinado
    y_pred = (models[ticker].predict(X_scaled) >= 0.5).astype(int).flatten()

    # Gera o relatório
    report = classification_report(y_full, y_pred, target_names=["Venda (-1)", "Compra (+1)"])
    print(report)

from sklearn.metrics import roc_curve, auc

for ticker, (X_train, y_train, X_test, y_test, X_full, y_full) in datasets.items():
    print(f"\n Curva ROC - {ticker}")

    # Padroniza os dados completos
    X_scaled = scalers[ticker].transform(X_full)

    # Obtém as probabilidades (não binarizadas)
    y_proba = models[ticker].predict(X_scaled).flatten()

    # Calcula pontos ROC
    fpr, tpr, _ = roc_curve(y_full, y_proba)
    roc_auc = auc(fpr, tpr)

    # Plot da curva ROC
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'Curva ROC - {ticker}')
    plt.legend(loc='lower right')
    plt.grid(False)
    plt.show()
    plt.savefig(f'roc_curve_{ticker}.png')

from sklearn.metrics import f1_score

# Lista de F1-scores
f1_scores = []

for ticker, (X_train, y_train, X_test, y_test, X_full, y_full) in datasets.items():
    print(f"Calculando F1-score para {ticker}")

    # Escalar os dados completos
    X_scaled = scalers[ticker].transform(X_full)

    # Predição binária
    y_pred = (models[ticker].predict(X_scaled) >= 0.5).astype(int).flatten()

    # Calcular F1-score
    f1 = f1_score(y_full, y_pred)

    f1_scores.append({'Ticker': ticker,
                      'F1-score': round(f1, 4)})

# Criar DataFrame final
df_f1 = pd.DataFrame(f1_scores).sort_values(by='F1-score', ascending=False)
df_f1

"""# **Parte 10 - Resultados final Rede Neural Classificação**"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Lista para armazenar os resultados
resultados = []

for ticker, (X_train, y_train, X_test, y_test, X_full, y_full) in datasets.items():
    print(f"Avaliando modelo para {ticker}")

    # Escalar os dados completos
    X_scaled = scalers[ticker].transform(X_full)

    # Probabilidades e predições binárias
    y_proba = models[ticker].predict(X_scaled).flatten()
    y_pred = (y_proba >= 0.5).astype(int)

    # Métricas
    acc  = accuracy_score(y_full, y_pred)
    prec = precision_score(y_full, y_pred, zero_division=0)
    rec  = recall_score(y_full, y_pred)
    f1   = f1_score(y_full, y_pred)
    auc  = roc_auc_score(y_full, y_proba)

    resultados.append({'Ticker': ticker,
                       'Modelo': 'Rede Neural MLP',
                       'Accuracy': round(acc, 4),
                       'Precision': round(prec, 4),
                       'Recall': round(rec, 4),
                       'F1-score': round(f1, 4),
                       'AUC': round(auc, 4)})

# DataFrame final
df_resultados = pd.DataFrame(resultados)
df_resultados = df_resultados.sort_values(by='F1-score', ascending=False)
df_resultados

"""# **Parte 11 - Rede Neural 2 - Série temporal**

- Vamos criar rede neural para empresas como VALE, Petrobras etc.
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, Dropout, LSTM, GRU, Conv1D, Flatten, Conv2D, MaxPooling2D, Reshape
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Dataset geral
bbas = pd.read_csv('/content/BBAS3_SA_treino.csv')
csna = pd.read_csv('/content/CSNA3_SA_treino.csv')
petr = pd.read_csv('/content/PETR4_SA_treino.csv')
vale = pd.read_csv('/content/VALE3_SA_treino.csv')

# VALE3
df_vale_train = pd.read_csv('/content/VALE3_SA_treino.csv')
df_vale_test  = pd.read_csv('/content/VALE3_SA_teste.csv')

# PETR4
df_petr_train = pd.read_csv('/content/PETR4_SA_treino.csv')
df_petr_test  = pd.read_csv('/content/PETR4_SA_teste.csv')

# BBAS3
df_bbas_train = pd.read_csv('/content/BBAS3_SA_treino.csv')
df_bbas_test  = pd.read_csv('/content/BBAS3_SA_teste.csv')

# CSNA3
df_csna_train = pd.read_csv('/content/CSNA3_SA_treino.csv')
df_csna_test  = pd.read_csv('/content/CSNA3_SA_teste.csv')

# Exemplo para VALE3
df_vale_train = df_vale_train.loc[:, ~df_vale_train.columns.str.contains('^Unnamed')]
df_vale_test  = df_vale_test.loc[:, ~df_vale_test.columns.str.contains('^Unnamed')]

print("VALE3:", df_vale_train.columns.tolist())
print("PETR4:", df_petr_train.columns.tolist())

# Função auxiliar para limpar
def limpar_df(df):
    return df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Aplicar nos datasets
df_vale_train = limpar_df(df_vale_train)
df_vale_test  = limpar_df(df_vale_test)

df_petr_train = limpar_df(df_petr_train)
df_petr_test  = limpar_df(df_petr_test)

df_bbas_train = limpar_df(df_bbas_train)
df_bbas_test  = limpar_df(df_bbas_test)

df_csna_train = limpar_df(df_csna_train)
df_csna_test  = limpar_df(df_csna_test)

print("VALE3:", df_vale_train.columns.tolist())
print("PETR4:", df_petr_train.columns.tolist())
print("BBAS3:", df_bbas_train.columns.tolist())
print("CSNA3:", df_csna_train.columns.tolist())

# VALE3
X_vale = df_vale_train[[f'Past_{i}_Days_Close' for i in range(1, 16)]].values
y_vale = df_vale_train['Close'].values

# PETR4
X_petr = df_petr_train[[f'Past_{i}_Days_Close' for i in range(1, 16)]].values
y_petr = df_petr_train['Close'].values

# BBAS3
X_bbas = df_bbas_train[[f'Past_{i}_Days_Close' for i in range(1, 16)]].values
y_bbas = df_bbas_train['Close'].values

# CSNA3
X_csna = df_csna_train[[f'Past_{i}_Days_Close' for i in range(1, 16)]].values
y_csna = df_csna_train['Close'].values

# Escalonamento

# VALE3
scaler_vale_X = StandardScaler()
scaler_vale_y = StandardScaler()
X_vale_scaled = scaler_vale_X.fit_transform(X_vale)
y_vale_scaled = scaler_vale_y.fit_transform(y_vale.reshape(-1, 1))

# PETR4
scaler_petr_X = StandardScaler()
scaler_petr_y = StandardScaler()
X_petr_scaled = scaler_petr_X.fit_transform(X_petr)
y_petr_scaled = scaler_petr_y.fit_transform(y_petr.reshape(-1, 1))

# BBAS3
scaler_bbas_X = StandardScaler()
scaler_bbas_y = StandardScaler()
X_bbas_scaled = scaler_bbas_X.fit_transform(X_bbas)
y_bbas_scaled = scaler_bbas_y.fit_transform(y_bbas.reshape(-1, 1))

# CSNA3
scaler_csna_X = StandardScaler()
scaler_csna_y = StandardScaler()
X_csna_scaled = scaler_csna_X.fit_transform(X_csna)
y_csna_scaled = scaler_csna_y.fit_transform(y_csna.reshape(-1, 1))

#
scaler_vale_X

# Treinamento

# VALE3
split_vale = int(0.8 * len(X_vale_scaled))
X_vale_train, X_vale_test = X_vale_scaled[:split_vale], X_vale_scaled[split_vale:]
y_vale_train, y_vale_test = y_vale_scaled[:split_vale], y_vale_scaled[split_vale:]

# PETR4
split_petr = int(0.8 * len(X_petr_scaled))
X_petr_train, X_petr_test = X_petr_scaled[:split_petr], X_petr_scaled[split_petr:]
y_petr_train, y_petr_test = y_petr_scaled[:split_petr], y_petr_scaled[split_petr:]

# BBAS3
split_bbas = int(0.8 * len(X_bbas_scaled))
X_bbas_train, X_bbas_test = X_bbas_scaled[:split_bbas], X_bbas_scaled[split_bbas:]
y_bbas_train, y_bbas_test = y_bbas_scaled[:split_bbas], y_bbas_scaled[split_bbas:]

# CSNA3
split_csna = int(0.8 * len(X_csna_scaled))
X_csna_train, X_csna_test = X_csna_scaled[:split_csna], X_csna_scaled[split_csna:]
y_csna_train, y_csna_test = y_csna_scaled[:split_csna], y_csna_scaled[split_csna:]

# Adiciona a dimensão extra (features=1)

#
X_vale_train = X_vale_train.reshape((X_vale_train.shape[0], X_vale_train.shape[1], 1))
X_vale_test  = X_vale_test.reshape((X_vale_test.shape[0], X_vale_test.shape[1], 1))

#
X_petr_train = X_petr_train.reshape((X_petr_train.shape[0], X_petr_train.shape[1], 1))
X_petr_test  = X_petr_test.reshape((X_petr_test.shape[0], X_petr_test.shape[1], 1))

#
X_bbas_train = X_bbas_train.reshape((X_bbas_train.shape[0], X_bbas_train.shape[1], 1))
X_bbas_test  = X_bbas_test.reshape((X_bbas_test.shape[0], X_bbas_test.shape[1], 1))

#
X_csna_train = X_csna_train.reshape((X_csna_train.shape[0], X_csna_train.shape[1], 1))
X_csna_test  = X_csna_test.reshape((X_csna_test.shape[0], X_csna_test.shape[1], 1))

"""# Part 12 - Treinamento Rede Neural LSTM"""

# Dicionário com os dados
datasets = {

            #
            'VALE3': (X_vale_train, y_vale_train, X_vale_test, y_vale_test, scaler_vale_y),

            #
            'PETR4': (X_petr_train, y_petr_train, X_petr_test, y_petr_test, scaler_petr_y),

            #
            'BBAS3': (X_bbas_train, y_bbas_train, X_bbas_test, y_bbas_test, scaler_bbas_y),

            #
            'CSNA3': (X_csna_train, y_csna_train, X_csna_test, y_csna_test, scaler_csna_y),
            }

#
resultados = []

#
modelos_dl = ['LSTM', 'GRU', 'CNN1D', 'CNN2D']

#
histories = {}

modelos = {}

for nome_modelo in modelos_dl:
    for ticker, (X_train, y_train, X_test, y_test, scaler_y) in datasets.items():
        print(f"\nTreinando modelo {nome_modelo} para {ticker}...\n")

        model2 = Sequential()

        if nome_modelo == 'LSTM':
            model2.add(LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2])))
            model2.add(Dropout(0.3))
        elif nome_modelo == 'GRU':
            model2.add(GRU(32, input_shape=(X_train.shape[1], X_train.shape[2])))
            model2.add(Dropout(0.3))
        elif nome_modelo == 'CNN1D':
            model2.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))
            model2.add(Flatten())
        elif nome_modelo == 'CNN2D':
            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1, 1))
            X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1))
            model2.add(Conv2D(16, kernel_size=(3,1), activation='relu', input_shape=(X_train.shape[1], 1, 1)))
            model2.add(MaxPooling2D(pool_size=(2,1)))
            model2.add(Flatten())

        model2.add(Dense(16, activation='relu'))
        model2.add(Dense(1))

        model2.compile(loss='mean_squared_error', optimizer=Adam(0.001), metrics=['mae'])
        es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

        history2 = model2.fit(X_train, y_train,
                              validation_data=(X_test, y_test),
                              epochs=100,
                              batch_size=32,
                              callbacks=[es],
                              verbose=1)

        chave = f"{nome_modelo}_{ticker}"
        histories[chave] = history2
        modelos[chave] = model2  # <- ESSENCIAL!

        # Previsão para métricas
        y_pred_scaled = model2.predict(X_test)
        y_pred = scaler_y.inverse_transform(y_pred_scaled)
        y_true = scaler_y.inverse_transform(y_test)

        resultados.append({
            "Ticker": ticker,
            "Modelo": nome_modelo,
            "MAE": mean_absolute_error(y_true, y_pred),
            "RMSE": np.sqrt(mean_squared_error(y_true, y_pred)),
            "R2": r2_score(y_true, y_pred)
        })

"""# Parte 13 - Métricas e avaliações"""

# Número total de gráficos (um por combinação modelo + ação)
n_graficos = len(histories)
ncols = 2  # uma coluna para Loss, outra para MAE
nrows = n_graficos

for chave, history in histories.items():
    # Tenta dividir a chave corretamente
    if '_' in chave:
        modelo, ticker = chave.split('_', 1)
    else:
        modelo = 'Desconhecido'
        ticker = chave

    hist = history.history
    epochs = range(1, len(hist['loss']) + 1)

    plt.figure(figsize=(14, 5))
    plt.suptitle(f'Desempenho da Rede Neural - {ticker} ({modelo})', fontsize=16)

    # LOSS
    plt.subplot(1, 2, 1)
    plt.plot(epochs, hist['loss'], label='Treino')
    if 'val_loss' in hist:
        plt.plot(epochs, hist['val_loss'], label='Validação')
    plt.title(f'Loss - {ticker}')
    plt.xlabel('Épocas')
    plt.ylabel('Loss')
    plt.legend()

    # MAE
    plt.subplot(1, 2, 2)
    if 'mae' in hist:
        plt.plot(epochs, hist['mae'], label='Treino')
        if 'val_mae' in hist:
            plt.plot(epochs, hist['val_mae'], label='Validação')
        plt.title(f'MAE - {ticker}')
        plt.xlabel('Épocas')
        plt.ylabel('MAE')
        plt.legend()
    else:
        plt.text(0.5, 0.5, 'MAE não disponível', ha='center', va='center', fontsize=12)
        plt.title(f'MAE - {ticker}')

    plt.tight_layout()
    plt.savefig(f'history2{ticker}.png')
    plt.show()

# Cria uma grade 4x4 (4 ações × 4 modelos)
fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(24, 20))
fig.suptitle("📈 Previsão dos Modelos vs. Valores Reais (Close)", fontsize=22)
plt.subplots_adjust(hspace=0.4, wspace=0.3)

for idx, (chave, model) in enumerate(modelos.items()):
    nome_modelo, ticker = chave.split('_')
    X_train, y_train, X_test, y_test, scaler_y = datasets[ticker]

    # Ajustar formato para CNN2D
    if nome_modelo == 'CNN2D':
        X_test_input = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1))
    else:
        X_test_input = X_test

    # Previsões
    y_pred_scaled = model.predict(X_test_input)
    y_pred = scaler_y.inverse_transform(y_pred_scaled)
    y_true = scaler_y.inverse_transform(y_test)

    # Posição do subplot
    row = idx // 4
    col = idx % 4
    ax = axes[row, col]

    # Plot
    ax.plot(y_true, label='Real', linewidth=2)
    ax.plot(y_pred, label='Previsto', linestyle='--')
    ax.set_title(f'{ticker} - {nome_modelo}', fontsize=12)
    ax.set_xlabel("Amostras")
    ax.set_ylabel("Preço Fechamento")
    ax.grid(True)
    ax.legend()

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.savefig("subplots_previsoes_modelos.png", bbox_inches='tight')
plt.savefig(f'Previsão_serie_temporal{ticker}.png')
plt.show()

# Criar DataFrame com os resultados
df_metricas = pd.DataFrame(resultados)

# Ordenar por Ticker e Modelo para melhor visualização
df_metricas = df_metricas.sort_values(by=["Ticker", "Modelo"]).reset_index(drop=True)
df_metricas

# Converte a lista de resultados em DataFrame
df_resultados = pd.DataFrame(resultados)

# Gráfico de barras comparando o desempenho (MAE, RMSE, R2)
fig, axes = plt.subplots(1, 3, figsize=(20, 6))

# MAE
sns.barplot(data=df_resultados, x='Modelo', y='MAE', hue='Ticker', ax=axes[0])
axes[0].set_title('Erro Absoluto Médio (MAE) por Modelo')

# RMSE
sns.barplot(data=df_resultados, x='Modelo', y='RMSE', hue='Ticker', ax=axes[1])
axes[1].set_title('Raiz do Erro Quadrático Médio (RMSE) por Modelo')

# R2
sns.barplot(data=df_resultados, x='Modelo', y='R2', hue='Ticker', ax=axes[2])
axes[2].set_title('R² por Modelo')

plt.tight_layout()
plt.show()

"""# **Parte 14 - Previsões Rede Neural 2 - Serie temporal**"""

previsoes = []

for chave, model in modelos.items():
    nome_modelo, ticker = chave.split('_')
    X_train, y_train, X_test, y_test, scaler_y = datasets[ticker]

    # CNN2D precisa de reshape
    X_test_input = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1)) if nome_modelo == 'CNN2D' else X_test

    y_pred_scaled = model.predict(X_test_input)
    y_pred = scaler_y.inverse_transform(y_pred_scaled)
    y_true = scaler_y.inverse_transform(y_test)

    for i in range(len(y_true)):
        previsoes.append({
            'Ticker': ticker,
            'Modelo': nome_modelo,
            'Amostra': i,
            'Real': y_true[i][0],
            'Previsto': y_pred[i][0]
        })

df_prev = pd.DataFrame(previsoes)

# Salvando dataset
df_prev.to_csv("previsoes_modelos.csv", index=False)

# VIsualizando dataset
df_prev.head(n=10)

"""# Parte 15 - Previsões base teste

- **Previsões com base de test**
"""

# Caminhos dos arquivos de teste
arquivos_teste = {
    'BBAS3': '/content/BBAS3_SA_teste.csv',
    'CSNA3': '/content/CSNA3_SA_teste.csv',
    'PETR4': '/content/PETR4_SA_teste.csv',
    'VALE3': '/content/VALE3_SA_teste.csv',
}

# Dicionários para armazenar os dados de teste processados
X_teste_final = {}
y_teste_final = {}
scalers_y_test = {}

window_size = 15  # dias anteriores usados

for ticker, caminho in arquivos_teste.items():
    df = pd.read_csv(caminho)

    # Supondo que a coluna alvo é 'Close'
    scaler_x = MinMaxScaler()
    scaler_y = MinMaxScaler()

    # Features e alvo
    features = ['Close']  # adicione mais se usou mais colunas no treino
    dados_x = scaler_x.fit_transform(df[features])
    dados_y = scaler_y.fit_transform(df[['Close']])

    # Criar janelas deslizantes (últimos 15 dias preveem o próximo)
    X, y = [], []
    for i in range(window_size, len(df)):
        X.append(dados_x[i-window_size:i])
        y.append(dados_y[i])

    X = np.array(X)
    y = np.array(y)

    X_teste_final[ticker] = X
    y_teste_final[ticker] = y
    scalers_y_test[ticker] = scaler_y

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
avaliacoes_teste = []

for chave, model in modelos.items():
    nome_modelo, ticker = chave.split('_')

    # Pegar dados de teste
    X_test_novo = X_teste_final[ticker]
    y_test_novo = y_teste_final[ticker]
    scaler_y = scalers_y_test[ticker]

    # Ajustar entrada para CNN2D
    if nome_modelo == 'CNN2D':
        X_input = X_test_novo.reshape((X_test_novo.shape[0], X_test_novo.shape[1], 1, 1))
    else:
        X_input = X_test_novo

    # Previsão
    y_pred_scaled = model.predict(X_input)
    y_pred = scaler_y.inverse_transform(y_pred_scaled)
    y_true = scaler_y.inverse_transform(y_test_novo)

    # Métricas
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    avaliacoes_teste.append({
        "Ticker": ticker,
        "Modelo": nome_modelo,
        "MAE": mae,
        "RMSE": rmse,
        "R2": r2
    })

# Mostrar resultados
df_avaliacoes = pd.DataFrame(avaliacoes_teste)
df_avaliacoes

# Cria uma grade 4x4 (4 ações × 4 modelos)
fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(24, 20))
fig.suptitle("📊 Previsões dos Modelos nos Dados de Teste vs. Valores Reais (Close)", fontsize=22)
plt.subplots_adjust(hspace=0.4, wspace=0.3)

for idx, (chave, model) in enumerate(modelos.items()):
    nome_modelo, ticker = chave.split('_')

    # Dados de teste
    X_test = X_teste_final[ticker]
    y_test = y_teste_final[ticker]
    scaler_y = scalers_y_test[ticker]

    # Ajustar entrada para CNN2D
    if nome_modelo == 'CNN2D':
        X_input = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1))
    else:
        X_input = X_test

    # Previsão e reversão da escala
    y_pred_scaled = model.predict(X_input)
    y_pred = scaler_y.inverse_transform(y_pred_scaled)
    y_true = scaler_y.inverse_transform(y_test)

    # Cálculo das métricas
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    # Subplot
    row = idx // 4
    col = idx % 4
    ax = axes[row, col]

    ax.plot(y_true, label='Real', linewidth=2)
    ax.plot(y_pred, label='Previsto', linestyle='--')
    ax.set_title(f'{ticker} - {nome_modelo}', fontsize=12)
    ax.set_xlabel("Amostras")
    ax.set_ylabel("Preço Fechamento")
    ax.grid(True)
    ax.legend()

    # Texto com as métricas no canto superior direito
    texto_metricas = f"MAE: {mae:.2f}\nRMSE: {rmse:.2f}\nR²: {r2:.2f}"
    ax.text(0.95, 0.05, texto_metricas,
            transform=ax.transAxes,
            verticalalignment='bottom',
            horizontalalignment='right',
            fontsize=10,
            bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.4'))

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.savefig("subplots_previsoes_teste_com_metricas.png", bbox_inches='tight')
plt.show()

"""# Parte 16 - Previsão no futuro (Foresquet)"""

# Número de passos à frente
n_futuro = 30

# Modelo e Ticker (exemplo: modelo LSTM para PETR4)
# 'GRU', 'CNN1D', 'CNN2D'
# Escolher modelo Rede Neural
nome_modelo = 'LSTM'
ticker = 'PETR4'

# Recuperar dados e modelo
X_train, y_train, X_test, y_test, scaler_y = datasets[ticker]
modelo = modelos[f"{nome_modelo}_{ticker}"]

# Ajustar formato para CNN2D
if nome_modelo == 'CNN2D':
    entrada_atual = X_test[-1].reshape(1, X_test.shape[1], 1, 1)
else:
    entrada_atual = X_test[-1].reshape(1, X_test.shape[1], X_test.shape[2])  # (1, time_steps, features)

# Lista para previsões futuras
previsoes_futuras = []

# Loop de previsão
for _ in range(n_futuro):
    pred_scaled = modelo.predict(entrada_atual, verbose=0)
    pred = scaler_y.inverse_transform(pred_scaled)
    previsoes_futuras.append(pred[0, 0])

    # Nova amostra (em formato compatível)
    if nome_modelo == 'CNN2D':
        nova_amostra = pred_scaled.reshape(1, 1, 1, 1)
        entrada_atual = np.concatenate([entrada_atual[:, 1:, :, :], nova_amostra], axis=1)
    else:
        nova_amostra = pred_scaled.reshape(1, 1, 1)
        entrada_atual = np.concatenate([entrada_atual[:, 1:, :], nova_amostra], axis=1)

# Reverter escala dos dados reais
y_test_real = scaler_y.inverse_transform(y_test)

# Supondo que você tenha as datas do conjunto de teste (ex: últimas N linhas do dataframe original)
datas_test = df['Date'].iloc[-len(y_test_real):].reset_index(drop=True)

# Última data real
ultima_data = pd.to_datetime(datas_test.iloc[-1])

# Gerar datas futuras (diárias ou conforme a sua base)
datas_futuras = pd.date_range(start=ultima_data + pd.Timedelta(days=1), periods=n_futuro, freq='B')  # 'B' = dias úteis

# Concatenar as datas para o eixo X
datas_x = pd.concat([datas_test, pd.Series(datas_futuras)])

# Concatenar os valores reais + previsão
valores_plot = np.concatenate([y_test_real.flatten(), previsoes_futuras])

# Certifique-se de que 'Date' está em datetime
df['Date'] = pd.to_datetime(df['Date'])

# Extrair datas da parte de teste
datas_test = df['Date'].iloc[-len(y_test_real):].reset_index(drop=True)

# Última data conhecida
ultima_data = datas_test.iloc[-1]

# Criar datas futuras (dias úteis)
datas_futuras = pd.date_range(start=ultima_data + pd.Timedelta(days=1), periods=n_futuro, freq='B')
datas_futuras

# Estilo Seaborn
sns.set(style="whitegrid")

# Figura
plt.figure(figsize=(14, 6))

# Série real com Seaborn
sns.lineplot(x=datas_test, y=y_test_real.flatten(), label='Real (Test)', color='royalblue', linewidth=2)

# Previsão futura com marcador customizado
plt.plot(datas_futuras, previsoes_futuras, 'r--o', label='Previsão Futura', linewidth=2, markersize=7)

# Título e rótulos
plt.title(f"📈 Previsão Futura com {nome_modelo} - {ticker}", fontsize=16, weight='bold')
plt.xlabel("Data", fontsize=12)
plt.ylabel("Preço de Fechamento", fontsize=12)

# Formatação do eixo X
plt.xticks(rotation=45)
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))

# Grade mais suave
plt.grid(True, linestyle='--', alpha=0.3)

# Legenda
plt.legend(fontsize=11)

# Layout final
plt.tight_layout()
plt.savefig("grafico_previsao_lstm.png", dpi=300)
plt.show()

"""# Part 17 - Salvando modelos RNN"""

from tensorflow.keras.models import load_model
import os
import joblib

# Cria as pastas se não existirem
os.makedirs("modelos_salvos", exist_ok=True)
os.makedirs("scalers_salvos", exist_ok=True)

# Salvar modelos e scalers por ticker
for ticker in models.keys():
    # Caminhos
    caminho_modelo = f"modelos_salvos/modelo_{ticker}.keras"
    caminho_scaler = f"scalers_salvos/scaler_{ticker}.pkl"

    # Salvar modelo
    models[ticker].save(caminho_modelo)
    print(f"Modelo salvo: {caminho_modelo}")

    # Salvar scaler
    joblib.dump(scalers[ticker], caminho_scaler)
    print(f"Scaler salvo: {caminho_scaler}")

# Cria uma pasta para salvar os modelos
os.makedirs("modelos_salvos", exist_ok=True)

# Salva cada modelo no formato .keras
for nome, modelo in modelos.items():
    caminho = os.path.join("modelos_salvos", f"{nome}.keras")
    modelo.save(caminho)
    print(f"Modelo salvo: {caminho}")

"""# Parte 18 - Conclusão"""

