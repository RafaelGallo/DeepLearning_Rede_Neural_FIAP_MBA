# -*- coding: utf-8 -*-
"""RedeNeural_previsao_açoes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17QSDGOW-Me13oU7cfFAKqqRomaPRLYWq

# **QuantumFinance – Deep Learning para Decisões no Mercado Financeiro**

![](https://sdmntprsouthcentralus.oaiusercontent.com/files/00000000-2b74-61f7-bb25-a7f51333e8fc/raw?se=2025-06-30T00%3A52%3A43Z&sp=r&sv=2024-08-04&sr=b&scid=c2cd711a-1e4b-5252-b0dd-1e6a54bb67a8&skoid=9ccea605-1409-4478-82eb-9c83b25dc1b0&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-06-29T19%3A12%3A01Z&ske=2025-06-30T19%3A12%3A01Z&sks=b&skv=2024-08-04&sig=SxrwCr93Ox23UayGCGxdXIT4mf8yn5Ba%2B9xwJMY2Nc8%3D)

# **Parte 1 - Problema de Negócio**

### Contexto

A QuantumFinance, uma gestora de recursos especializada em inovação e tecnologia, está planejando lançar um fundo de ações 100% baseado em **modelos de Deep Learning**. O objetivo é construir um sistema automatizado de compra e venda de ações, buscando rentabilidade superior ao mercado usando inteligência artificial.

Para iniciar este projeto, foram selecionadas **quatro ações brasileiras altamente negociadas**:

* **VALE3** — Vale S.A. (Mineração)
* **PETR4** — Petrobras (Petróleo & Energia)
* **BBAS3** — Banco do Brasil (Financeiro)
* **CSNA3** — Companhia Siderúrgica Nacional (Siderurgia)

### Objetivo do Projeto

O objetivo central é **desenvolver modelos de Deep Learning capazes de identificar tendências de alta e baixa** no preço dessas ações, automatizando a decisão de **comprar** ou **vender** cada papel diariamente. O foco é construir um “**perseguidor de tendência**”:

* **Compra** quando o papel está em tendência de alta
* **Venda** quando o papel está em tendência de baixa

Essas decisões devem ser tomadas **automaticamente** com base apenas nos dados históricos recentes do mercado.

### Detalhes do Desafio

#### 1. **Rotulagem & Dados**

* Cada ação possui dados históricos diários de preços (de 2000 a 2023).
* Especialistas rotularam, dia a dia, se a ação deveria ser **comprada** (+1) ou **vendida** (-1) no dia seguinte, considerando a tendência dos últimos 15 dias.
* Os dados são fornecidos em dois formatos:

  * **Tabular (CSV)**: Cada linha contém a data, preço de fechamento filtrado, rótulo de compra/venda e as séries dos 15 dias anteriores.
  * **Imagens (PNG)**: Gráficos de barras normalizados dos últimos 15 dias para cada dia (permitem uso de CNN 2D).

#### 2. **Premissas**

* A estratégia do fundo depende da precisão dos modelos para identificar corretamente as tendências.
* Todas as decisões precisam ser baseadas **apenas em informações disponíveis até o dia anterior** (problema real de predição/robustez).

#### 3. **Requisitos do Projeto**

* Treinar **modelos de Deep Learning** para cada ação.

  * Sugerido: CNN 1D/2D, RNN (LSTM/GRU), modelos híbridos, etc.
* O sistema deve indicar diariamente: **COMPRAR** (+1) ou **VENDER** (-1).
* O modelo precisa ser entregue em **Jupyter Notebook**, incluindo:

  * **Acurácia** no teste.
  * **Matriz de confusão**, **precision**, **recall**.
  * **Backtest financeiro**: simulação do lucro se o modelo fosse usado para operar de fato.

#### 4. **Avaliação do Projeto**

* **Resultados quantitativos**: métricas de classificação e desempenho financeiro (lucro, drawdown, etc.).
* **Diferenciais inovadores** são incentivados: uso de arquiteturas avançadas, ensembles, integração de imagens com dados tabulares, explainability, dashboards interativos.
*
### Motivação de Negócio

* **Automatização inteligente** das operações para aumentar a eficiência e potencializar os lucros do fundo.
* **Redução do viés humano**, permitindo decisões consistentes e baseadas em dados.
* **Adoção de IA de ponta** para construir um diferencial competitivo no setor financeiro.

### Exemplos de Questões a serem Respondidas

* O modelo consegue aprender padrões de tendência nos preços das ações?
* O sistema realmente gera lucro financeiro consistente ao longo do tempo?
* Como o desempenho do modelo se compara a estratégias simples (ex: buy & hold)?
* É possível aprimorar o modelo integrando múltiplos tipos de entrada (dados tabulares + imagens)?

## Tecnologias e Abordagens Utilizadas

### 1. **Redes Neurais para Análise de Séries Temporais e Padrões Visuais**

Para resolver o desafio proposto, serão empregadas as seguintes tecnologias de Deep Learning, escolhidas por sua capacidade de identificar padrões complexos em séries históricas e dados visuais:

#### **a) LSTM (Long Short-Term Memory)**

* **O que é:** Uma arquitetura de rede neural recorrente (RNN) capaz de aprender dependências de longo prazo em sequências temporais.
* **Aplicação no projeto:** Permite que o modelo “lembre” dos movimentos dos preços ao longo dos últimos 15 dias, capturando tendências e reações típicas do mercado.
* **Benefício para o negócio:** Maior precisão na detecção de tendências e pontos de reversão, aumentando o potencial de acerto das operações.

#### **b) GRU (Gated Recurrent Unit)**

* **O que é:** Variante simplificada da LSTM, com desempenho semelhante e treinamento mais rápido em alguns cenários.
* **Aplicação no projeto:** Ótima opção para testar sequências temporais, podendo ser combinada em modelos híbridos.
* **Benefício para o negócio:** Eficiência computacional e boa capacidade preditiva, acelerando experimentação e implantação.

#### **c) CNN 1D (Convolutional Neural Network 1D)**

* **O que é:** Rede neural que aprende padrões locais em sequências temporais (por exemplo, saltos bruscos, reversões ou padrões recorrentes nos preços).
* **Aplicação no projeto:** Explora comportamentos específicos dentro da janela de 15 dias.
* **Benefício para o negócio:** Complementa as redes recorrentes, identificando sinais rápidos ou mudanças abruptas que possam indicar oportunidades de trade.

#### **d) CNN 2D (Convolutional Neural Network 2D)**

* **O que é:** Rede neural poderosa para análise de imagens.
* **Aplicação no projeto:** Usada para analisar gráficos PNG dos últimos 15 dias, permitindo ao modelo capturar padrões visuais que especialistas humanos muitas vezes identificam visualmente.
* **Benefício para o negócio:** Abre espaço para inovação, aproveitando representações visuais dos dados para melhorar a tomada de decisão.

### 2. **Análise de Dados e Pipeline de Modelagem**

* **Pré-processamento:** Limpeza, normalização/padronização de dados e organização de janelas deslizantes para os modelos sequenciais.
* **Feature Engineering:** Extração automática de padrões e relações temporais/pictóricas relevantes para previsão.
* **Avaliação Rigorosa:** Utilização de métricas clássicas (acurácia, precisão, recall, matriz de confusão) e **backtest financeiro** simulando operações reais.
* **Experimentação com Híbridos e Ensembles:** Combinação dos melhores modelos para potencializar resultados.
* **Explainability:** Análise de importância das features e visualização das ativações para auditoria do comportamento dos modelos.
* **Implementação em Jupyter Notebook:** Documentação, reprodutibilidade e transparência.

## Resumindo:

> A QuantumFinance utilizará o que há de mais moderno em inteligência artificial aplicada ao mercado financeiro, integrando redes neurais especializadas (LSTM, GRU, CNN 1D e 2D) e técnicas avançadas de análise de dados para construir um sistema robusto, inteligente e voltado para resultados reais.

### Impacto Esperado

* **Se bem-sucedido, o modelo pode ser usado como base para estratégias reais de investimento**, aumentando a rentabilidade dos fundos da QuantumFinance e abrindo portas para novos produtos baseados em IA.

## Resumão Visual do Problema

> **Problema:**

> Prever diariamente, com base nos últimos 15 dias, se cada ação selecionada deve ser comprada ou vendida, usando Deep Learning, maximizando o lucro do fundo via automação de decisões.

# **Parte 2 - Importando bibliotecas**
"""

import os
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

"""# Parte 3 - Verificação GPU disponível"""

# Mostra informações da GPU disponível (no Colab/Jupyter, use !nvidia-smi)
!nvidia-smi

# Importa as principais bibliotecas de Deep Learning
import keras
import tensorflow as tf

# Exibe a versão do TensorFlow e as GPUs detectadas pelo TensorFlow
print("Versão do TensorFlow:", tf.__version__)
print("GPUs disponíveis:", tf.config.list_physical_devices('GPU'))

# Verifica se o TensorFlow encontrou alguma GPU e exibe mensagem correspondente
if tf.config.list_physical_devices('GPU'):
    print("✅ GPU encontrada e disponível!")
else:
    print("❌ Nenhuma GPU encontrada. Usando apenas CPU.")

# Verificar GPU disponível com Keras
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

"""# **Parte 4 - Base dados**"""

# Base dados - BBAS3_SA
BBAS3_SA_train = '/content/BBAS3_SA_treino.csv'
BBAS3_SA_test = '/content/BBAS3_SA_teste.csv'

# Base dados - CSNA3_SA
CSNA3_SA_train = '/content/CSNA3_SA_treino.csv'
CSNA3_SA_test = '/content/CSNA3_SA_teste.csv'

# Base dados - PETR4_SA
PETR4_SA_train = '/content/PETR4_SA_treino.csv'
PETR4_SA_test = '/content/PETR4_SA_teste.csv'

# Base dados - VALE3_SA
VALE3_SA_train = '/content/VALE3_SA_treino.csv'
VALE3_SA_test = '/content/VALE3_SA_teste.csv'

# Função auxiliar para carregar e limpar (remover Unnamed: 0)
def carrega_limpa_csv(caminho):
    return pd.read_csv(caminho, usecols=lambda c: c != 'Unnamed: 0')

## Carregando dataset
# BBAS3
df_train_bbas = carrega_limpa_csv(BBAS3_SA_train)
df_test_bbas  = carrega_limpa_csv(BBAS3_SA_test)

# CSNA3
df_train_csna = carrega_limpa_csv(CSNA3_SA_train)
df_test_csna  = carrega_limpa_csv(CSNA3_SA_test)

# PETR4
df_train_petr = carrega_limpa_csv(PETR4_SA_train)
df_test_petr  = carrega_limpa_csv(PETR4_SA_test)

# VALE3
df_train_vale = carrega_limpa_csv(VALE3_SA_train)
df_test_vale  = carrega_limpa_csv(VALE3_SA_test)

"""# **Parte 4.1 - Processamento dados**"""

# Carregar as bases
df_bbas = pd.read_csv(BBAS3_SA_train)
df_csna = pd.read_csv(CSNA3_SA_train)
df_petr = pd.read_csv(PETR4_SA_train)
df_vale = pd.read_csv(VALE3_SA_train)

# Renomear colunas para evitar conflito, exceto Unnamed: 0
df_bbas = df_bbas.add_prefix('BBAS3_')
df_csna = df_csna.add_prefix('CSNA3_')
df_petr = df_petr.add_prefix('PETR4_')
df_vale = df_vale.add_prefix('VALE3_')

# Carregar bases
dfs = []

#
for ticker, path in [('BBAS3', BBAS3_SA_train),
                     ('CSNA3', CSNA3_SA_train),
                     ('PETR4', PETR4_SA_train),
                     ('VALE3', VALE3_SA_train)]:

    #
    df = pd.read_csv(path)

    #
    df['Ticker'] = ticker

    #
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

    #
    dfs.append(df)

# Concatenar tudo (long format)
data = pd.concat(dfs, ignore_index=True)

# (Opcional) Organizar colunas
cols = ['Ticker', 'Date'] + [col for col in data.columns if col not in ['Ticker','Date']]
data = data[cols]

# Visualizando dataset
data

"""**OBS:** **Nesse merge foi feito ajutando todos os datasets para ser usado para analise exploratoria de dados não ser aplicado para modelos de redes neurais !**

# **Parte 5 - Limpeza de dados**
"""

# 1. Conferir dados faltantes
print("Dados faltantes (treino):")
print(data.isnull().sum())

# 4. Conferir tipos de dados
print(data.dtypes)

#
data = data.drop(columns=['Unnamed: 0'])

#
data

# Remove duplicatas (caso existam)
data = data.drop_duplicates()

# Checa se só tem labels válidos (+1 ou -1)
assert set(data['Label'].unique()).issubset({-1, 1}), "Atenção: Label inválido detectado!"

# (Opcional) Conferir valores negativos
for col in [c for c in data.columns if 'Close' in c]:
    if (data[col] < 0).any():
        print(f"Atenção: valores negativos em {col}")

# Ordena por data (se tiver coluna 'Data')
if 'Data' in data.columns:
    data = data.sort_values('Data').reset_index(drop=True)

# Pronto para o pipeline do modelo!
print("Base de dados limpa e pronta.")

"""# **Parte 6 - Analise exploratoria de dados**

**Perguntas de negocio resolvendo com Análise de dados**

1. **Como evoluiu o preço de fechamento de cada ação ao longo do tempo?**

   → **Identifica tendências, crises e períodos de alta.**

2. **Qual a proporção de sinais de compra (+1) e venda (-1) para cada ação?**

   → **Avalia o balanço dos sinais emitidos por ativo.**

3. **A média móvel de 5 dias (MA\_5) é um bom indicador para os sinais de compra/venda?**

   → **Proporção de dias em que Label=1 e Close > MA\_5, e vice-versa.**

4. **Quando o preço cruza para cima da média móvel de 15 dias (MA\_15), há maior frequência de sinais de compra?**

   → **Relaciona cruzamentos com a coluna Label.**

5. **Existe diferença significativa na volatilidade diária (Close) entre as ações?**

   → **Usa desvio padrão/boxplot de variação percentual diária.**

6. **A estratégia de cruzamento de médias móveis (MA\_5 e MA\_15) teria gerado mais sinais de compra ou de venda?**

   → **Quantifica cruzamentos de alta e baixa.**

7. **Qual ação teve maior número de cruzamentos de médias móveis nos últimos 10 anos?**

   → **Mede a frequência de cruzamentos, um sinal de maior volatilidade.**

8. **Qual o retorno acumulado de uma estratégia simples: comprar quando Close > MA\_5 e vender quando Close < MA\_5?**

   → **Simula um backtest e compara com buy & hold.**

9. **Em quantos dias o preço fechou acima de ambas as médias móveis (MA\_5 e MA\_15) para cada ação?**

   → **Indica possíveis tendências de força.**

10. **Os sinais de compra/venda (Label) ocorrem mais frequentemente próximos a máximas ou mínimas móveis de 15 dias?**

    → **Relaciona Label com extremos móveis.**

**Qual a proporção de sinais de compra e venda (Label) para cada ativo?**

- **Ver se algum ativo tem tendência maior para compra ou venda ao longo do tempo.**
"""

data.groupby('Ticker')['Label'].value_counts(normalize=True).unstack().plot(kind='bar', stacked=True)
plt.title('Proporção de sinais de compra e venda por ativo')
plt.ylabel('Proporção')
plt.show()

"""**O sinal de Label (+1/-1) realmente antecipa o movimento do preço no dia seguinte?**

- **Validar o poder preditivo do rótulo fornecido.**
"""

data['Next_Close'] = data.groupby('Ticker')['Close'].shift(-1)
data['Movimento'] = np.where(data['Next_Close'] > data['Close'], 1, -1)
acuracia = (data['Label'] == data['Movimento']).mean()
print(f'Acurácia do label em prever o movimento do próximo fechamento: {acuracia:.2%}')

"""**Como se comportam as variações percentuais diárias dos ativos? (Volatilidade)**

- **Qual ação é mais volátil? Qual tem dias de variação extrema ?**
"""

data['Variação_%'] = data.groupby('Ticker')['Close'].pct_change() * 100
sns.boxplot(data=data, x='Ticker', y='Variação_%')
plt.title('Distribuição da variação percentual diária por ativo')
plt.ylabel('Variação diária (%)')
plt.show()

"""**Questão 1)** Como evoluiu o preço de fechamento de cada ação ao longo do tempo?

- **Identifica tendências, crises e períodos de alta.**
"""

plt.figure(figsize=(14,6))

# Paleta customizada para os tickers
cores = {'BBAS3': '#1f77b4', 'CSNA3': '#ff7f0e', 'PETR4': '#2ca02c', 'VALE3': '#d62728'}

# Plot customizado com Seaborn
sns.lineplot(data=data, x='Date', y='Close', hue='Ticker', palette=cores, linewidth=2.2)

plt.title('Evolução do Preço de Fechamento por Ação (2000–2019)', fontsize=18, weight='bold')
plt.ylabel('Preço de Fechamento (R$)', fontsize=15)
plt.xlabel('Data', fontsize=14)
plt.legend(title='Ticker', title_fontsize=13, fontsize=12, loc='center left', bbox_to_anchor=(1.01, 0.5), frameon=False)
plt.grid(axis='y', linestyle='--', alpha=0.4)

# Deixar ticks de ano em ano
plt.gca().xaxis.set_major_locator(mdates.YearLocator())
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
plt.xticks(rotation=45)
plt.tight_layout()

# Remove as bordas superiores e à direita
sns.despine()

plt.show()

"""**Questão 2)** Qual a proporção de sinais de compra (+1) e venda (-1) para cada ação?

- **Avalia o balanço dos sinais emitidos por ativo.**
"""

# Redefine a ordem para garantir: Venda (-1) embaixo, Compra (+1) em cima
df_prop = data.groupby('Ticker')['Label'].value_counts(normalize=True).unstack()[[-1,1]]

fig, ax = plt.subplots(figsize=(8,5))
bars = df_prop.plot(kind='bar', stacked=True, color=['#E77D11', '#53A548'], ax=ax, width=0.7, edgecolor='k')

# Adiciona porcentagens nas barras
for i, ticker in enumerate(df_prop.index):
    v_pct = df_prop.loc[ticker, -1]
    c_pct = df_prop.loc[ticker, 1]
    ax.text(i, v_pct/2, f"{v_pct:.1%}", ha='center', va='center', color='white', fontsize=13, fontweight='bold')
    ax.text(i, v_pct + c_pct/2, f"{c_pct:.1%}", ha='center', va='center', color='white', fontsize=13, fontweight='bold')

plt.title('Proporção de Sinais de Compra (+1) e Venda (-1) por Ação', fontsize=15, weight='bold')
plt.ylabel('Proporção', fontsize=12)
plt.xlabel('Ação', fontsize=12)
plt.xticks(rotation=0, fontsize=11)
plt.yticks(fontsize=11)
plt.grid(axis='y', linestyle='--', alpha=0.3)

# Legenda fora do gráfico, sem borda
plt.legend(['Venda (-1)', 'Compra (+1)'], loc='center left', bbox_to_anchor=(1.01, 0.5), frameon=False, fontsize=12)
plt.tight_layout()
plt.show()

"""**Questão 3)** A média móvel de 5 dias (MA_5) é um bom indicador para os sinais de compra/venda?

- Proporção de dias em que Label=1 e Close > MA_5, e vice-versa.
"""

data['MA_5'] = data.groupby('Ticker')['Close'].transform(lambda x: x.rolling(5).mean())
compra_ma5 = ((data['Label'] == 1) & (data['Close'] > data['MA_5'])).mean()
venda_ma5  = ((data['Label'] == -1) & (data['Close'] < data['MA_5'])).mean()

# Proporções já calculadas
props = [compra_ma5, venda_ma5]
labels = [
    'Compra (+1) com Close > MA_5',
    'Venda (-1) com Close < MA_5'
]
colors = ['#53A548', '#E77D11']

plt.figure(figsize=(7,5))
bars = plt.bar(labels, props, color=colors, edgecolor='k', width=0.6)

# Adiciona valores percentuais nas barras
for bar, value in zip(bars, props):
    plt.text(bar.get_x() + bar.get_width()/2, value + 0.01, f"{value:.1%}", ha='center', va='bottom', fontsize=14, fontweight='bold')

plt.title('Alinhamento dos Sinais de Compra/Venda com MA_5', fontsize=15, weight='bold')
plt.ylabel('Proporção dos Dias', fontsize=12)
plt.ylim(0, 1)
plt.xticks(fontsize=12, rotation=10)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.3)
plt.tight_layout()
plt.show()

print(f"Proporção de dias Label=1 e Close > MA_5: {compra_ma5:.2%}")
print(f"Proporção de dias Label=-1 e Close < MA_5: {venda_ma5:.2%}")

"""**Questão 4)** Quando o preço cruza para cima da média móvel de 15 dias (MA_15), há maior frequência de sinais de compra ?

- Relaciona cruzamentos com a coluna Label.
"""

# 4. Cruzamento de alta com MA_15 e frequência de Label=1
data['MA_15'] = data.groupby('Ticker')['Close'].transform(lambda x: x.rolling(15).mean())
data['MA_5_prev'] = data.groupby('Ticker')['MA_5'].shift(1)
data['MA_15_prev'] = data.groupby('Ticker')['MA_15'].shift(1)
data['cruzou_cima'] = ((data['MA_5_prev'] < data['MA_15_prev']) & (data['MA_5'] >= data['MA_15']))
cruzamentos_compra = data[data['cruzou_cima']]['Label'].value_counts()

# Dados dos cruzamentos
cruzamentos = cruzamentos_compra.sort_index()  # Garante ordem -1, 1
labels = ['Venda (-1)', 'Compra (+1)']
colors = ['#E77D11', '#53A548']

plt.figure(figsize=(10, 5))
bars = plt.bar(labels, cruzamentos.values, color=colors, edgecolor='k', width=0.5)

# Adiciona os valores absolutos nas barras
for bar, value in zip(bars, cruzamentos.values):
    plt.text(bar.get_x() + bar.get_width()/2, value + 5, str(value), ha='center', va='bottom', fontsize=13, fontweight='bold')

plt.title('Frequência de Sinais após Cruzamento de Alta (MA_5 ↑ MA_15)', fontsize=14, weight='bold')
plt.ylabel('Número de Ocorrências', fontsize=12)
plt.ylim(0, max(cruzamentos.values)*1.15)
plt.xticks(fontsize=12)
plt.yticks(fontsize=11)
plt.tight_layout()
plt.show()

print("\nFrequência de Label após cruzamento de alta (MA_5 cruzou MA_15 para cima):")
print(cruzamentos_compra)

"""**Questão 5)** Existe diferença significativa na volatilidade diária (Close) entre as ações?

- Usa desvio padrão/boxplot de variação percentual diária.
"""

# Diferença de volatilidade entre ações (boxplot variação % diária)
plt.figure(figsize=(10,6))
sns.set_style('whitegrid')

# Ordem dos tickers pela mediana da volatilidade
ordem = (
    data.groupby('Ticker')['Variação_%']
    .median()
    .sort_values(ascending=False)
    .index
)

# Plot principal
ax = sns.boxplot(
    data=data,
    x='Ticker',
    y='Variação_%',
    order=ordem,
    palette='pastel',
    showfliers=False,
    linewidth=2
)

# Mediana e média anotadas
for i, ticker in enumerate(ordem):
    mediana = data.loc[data['Ticker']==ticker, 'Variação_%'].median()
    media   = data.loc[data['Ticker']==ticker, 'Variação_%'].mean()
    plt.scatter(i, mediana, color='blue', marker='o', s=70, zorder=10, label='Mediana' if i==0 else "")
    plt.scatter(i, media, color='red', marker='X', s=70, zorder=10, label='Média' if i==0 else "")
    plt.text(i+0.05, mediana, f"{mediana:.2f}", color='blue', fontsize=10, va='center', fontweight='bold')
    plt.text(i+0.05, media, f"{media:.2f}", color='red', fontsize=10, va='center')

# Ajustes estéticos
plt.title('Volatilidade Diária (% de Variação) por Ação', fontsize=16, weight='bold')
plt.ylabel('Variação diária (%)', fontsize=13)
plt.xlabel('Ticker', fontsize=13)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.xticks(fontsize=12)
plt.yticks(fontsize=11)
plt.legend(loc='upper right', fontsize=11, frameon=False)
plt.tight_layout()

# Nota de rodapé (opcional)
n_obs = data['Ticker'].value_counts().to_dict()
nota = " | ".join([f"{tkr}: {n} dias" for tkr, n in n_obs.items()])
plt.figtext(0.5, -0.05, f"Número de observações por ativo: {nota}", ha='center', fontsize=10, color='gray')

plt.show()

"""**Questão 6)** A estratégia de cruzamento de médias móveis (MA_5 e MA_15) teria gerado mais sinais de compra ou de venda ?

- Quantifica cruzamentos de alta e baixa.
"""

# 6. Quantidade de cruzamentos de alta/baixa (MA_5 e MA_15)
data['cruzou_baixo'] = ((data['MA_5_prev'] > data['MA_15_prev']) & (data['MA_5'] <= data['MA_15']))

# Se ainda não fez, garanta que está agrupando por ticker!
cruzamentos = (
    data.groupby('Ticker')[['cruzou_cima', 'cruzou_baixo']]
    .sum()
    .rename(columns={'cruzou_cima': 'Alta (MA_5↑MA_15)', 'cruzou_baixo': 'Baixa (MA_5↓MA_15)'})
    .astype(int)
)
print(cruzamentos)

print("\nCruzamentos de alta (MA_5↑MA_15):", data['cruzou_cima'].sum())
print("Cruzamentos de baixa (MA_5↓MA_15):", data['cruzou_baixo'].sum())

# Tamanho maior, ex: (12, 7) ou (14, 7)
fig, ax = plt.subplots(figsize=(12, 7))  # Altere para o tamanho desejado

# Plotando novamente (use o mesmo DataFrame cruzamentos)
cruzamentos.plot(
    kind='bar',
    color=['#53A548', '#E77D11'],
    width=0.8,
    ax=ax
)

plt.title('Quantidade de Cruzamentos de Alta e Baixa por Ação (MA_5 x MA_15)', fontsize=9, weight='bold')
plt.xlabel('Ticker', fontsize=15)
plt.ylabel('Nº de Cruzamentos', fontsize=15)
plt.xticks(rotation=0, fontsize=13)
plt.yticks(fontsize=13)
plt.grid(axis='y', linestyle='--', alpha=0.3)

# Legenda fora do gráfico
plt.legend(
    title='Tipo de Cruzamento',
    bbox_to_anchor=(1.02, 1),
    loc='upper left',
    borderaxespad=0,
    frameon=False,
    fontsize=13,
    title_fontsize=15
)

plt.tight_layout(rect=[0, 0, 0.87, 1])  # Ajuste para não cortar a legenda
plt.show()

"""**Questão 7)** Qual ação teve maior número de cruzamentos de médias móveis nos últimos 10 anos?

- Mede a frequência de cruzamentos, um sinal de maior volatilidade.
"""

# Define o ano máximo e filtra os últimos 10 anos
ultimo_ano = data['Date'].max().year
df10 = data[data['Date'] >= f'{ultimo_ano-9}-01-01']

# Calcula os cruzamentos de alta (MA_5 cruzou MA_15 para cima) por ativo
cruzamentos_por_ativo = df10.groupby('Ticker')['cruzou_cima'].sum()

# Ordena para visualização
cruzamentos_plot = cruzamentos_por_ativo.sort_values(ascending=False)

plt.figure(figsize=(20.5, 10))
bars = plt.bar(cruzamentos_plot.index, cruzamentos_plot.values, color='#2369B2')

plt.title('Cruzamentos de Alta (MA_5↑MA_15) nos Últimos 10 Anos por Ativo', fontsize=16, weight='bold')
plt.ylabel('Nº de Cruzamentos de Alta')
plt.xlabel('Ticker')

# Índice da maior barra
max_idx = cruzamentos_plot.values.argmax()

# Adiciona valor acima das barras (menos na maior)
for i, valor in enumerate(cruzamentos_plot.values):
    if i != max_idx:
        plt.text(
            cruzamentos_plot.index[i],
            valor + 1.5,
            f"{valor}",
            ha='center',
            va='bottom',
            fontsize=13,
            fontweight='bold',
            color='#444'
        )

# Destaca a maior barra e adiciona valor
bars[max_idx].set_color('#F99B24')
plt.text(
    cruzamentos_plot.index[max_idx],
    cruzamentos_plot.values[max_idx] + 4,
    f"{cruzamentos_plot.values[max_idx]}",
    color='#F99B24',
    fontsize=15,
    fontweight='bold',
    ha='center'
)

plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

print("\nCruzamentos de alta nos últimos 10 anos por ativo:")
print(cruzamentos_por_ativo)
print("Ativo com mais cruzamentos:", cruzamentos_por_ativo.idxmax())

"""**Questão 8)** Qual o retorno acumulado de uma estratégia simples: comprar quando Close > MA_5 e vender quando Close < MA_5?

- Simula um backtest e compara com buy & hold.
"""

# 8. Retorno acumulado da estratégia (Close > MA_5 = compra, <MA_5 = venda)
data['sinal'] = np.where(data['Close'] > data['MA_5'], 1, -1)
data['retorno'] = data.groupby('Ticker')['Close'].pct_change()
data['retorno_estrategia'] = data['retorno'] * data['sinal'].shift(1)

for ticker in data['Ticker'].unique():
    df_t = data[data['Ticker'] == ticker].copy()
    ret = (1 + df_t['retorno_estrategia'].fillna(0)).cumprod() - 1
    ret_bh = (1 + df_t['retorno'].fillna(0)).cumprod() - 1

    plt.figure(figsize=(14,5))
    plt.plot(df_t['Date'], 100*ret, label='Estratégia MA_5', linewidth=2, color='#2369B2')
    plt.plot(df_t['Date'], 100*ret_bh, label='Buy & Hold', linestyle='--', linewidth=2, color='#F99B24')

    plt.axhline(0, color='gray', linestyle=':', linewidth=1)
    plt.title(f'Retorno Acumulado (%) - {ticker}', fontsize=15, weight='bold')
    plt.xlabel('Data', fontsize=13)
    plt.ylabel('Retorno acumulado (%)', fontsize=13)
    plt.legend(fontsize=12, frameon=False, loc='upper left')

    # Mostra o valor final do retorno no gráfico
    plt.text(df_t['Date'].iloc[-1], 100*ret.iloc[-1], f"{100*ret.iloc[-1]:.1f}%", color='#2369B2', fontsize=12, va='center', ha='right', weight='bold')
    plt.text(df_t['Date'].iloc[-1], 100*ret_bh.iloc[-1], f"{100*ret_bh.iloc[-1]:.1f}%", color='#F99B24', fontsize=12, va='center', ha='right', weight='bold')

    # Datas: ticks por ano
    plt.gca().xaxis.set_major_locator(mdates.YearLocator())
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))

    plt.grid(axis='y', linestyle='--', alpha=0.4)
    plt.tight_layout()
    plt.show()

"""**Questão 9)** Em quantos dias o preço fechou acima de ambas as médias móveis (MA_5 e MA_15) para cada ação?

- Indica possíveis tendências de força.
"""

# 9. Dias com preço acima de MA_5 e MA_15 (por ativo)
acima_ma = data[(data['Close'] > data['MA_5']) & (data['Close'] > data['MA_15'])]
acima_ma_count = acima_ma.groupby('Ticker').size()

# Ordenar para visualização mais clara
acima_ma_count = acima_ma_count.sort_values(ascending=False)

plt.figure(figsize=(20.5, 10))
bars = plt.bar(acima_ma_count.index, acima_ma_count.values, color='#2369B2')

# Destaca o maior valor
max_idx = acima_ma_count.values.argmax()
bars[max_idx].set_color('#F99B24')

# Rótulo acima de cada barra
for i, valor in enumerate(acima_ma_count.values):
    plt.text(
        acima_ma_count.index[i],
        valor + 25,
        f"{valor:,}",
        ha='center',
        va='bottom',
        fontsize=13,
        fontweight='bold',
        color='#444' if i != max_idx else '#F99B24'
    )

plt.title('Dias com Preço Acima de MA_5 e MA_15 por Ação', fontsize=15, weight='bold')
plt.ylabel('Nº de Dias')
plt.xlabel('Ticker')
plt.grid(axis='y', linestyle='--', alpha=0.3)
plt.tight_layout()
plt.show()

print('\nDias com preço acima das duas médias móveis:')
print(acima_ma_count)

"""**Questão 10)** Os sinais de compra/venda (Label) ocorrem mais frequentemente próximos a máximas ou mínimas móveis de 15 dias?

- Relaciona Label com extremos móveis.
"""

# 10. Sinais próximos de máximas/mínimas móveis de 15 dias
data['max_15'] = data.groupby('Ticker')['Close'].transform(lambda x: x.rolling(15).max())
data['min_15'] = data.groupby('Ticker')['Close'].transform(lambda x: x.rolling(15).min())
prox_max = ((data['Label'] == 1) & (np.isclose(data['Close'], data['max_15'], atol=0.01))).sum()
prox_min = ((data['Label'] == -1) & (np.isclose(data['Close'], data['min_15'], atol=0.01))).sum()

# Valores dos sinais
valores = [prox_max, prox_min]
labels = ['Compra próxima à máxima móvel (MAx_15)', 'Venda próxima à mínima móvel (MIn_15)']
colors = ['#53A548', '#E77D11']

plt.figure(figsize=(9, 3.8))
bars = plt.barh(labels, valores, color=colors)

# Adiciona o valor ao lado da barra
for i, v in enumerate(valores):
    plt.text(
        v + 30, i,
        f"{v:,}",
        va='center',
        fontweight='bold',
        fontsize=13,
        color=colors[i]
    )

plt.title('Sinais Próximos de Máximas/Mínimas Móveis de 15 Dias', fontsize=15, weight='bold')
plt.xlabel('Quantidade de Sinais')
plt.xlim(0, max(valores) + 300)
plt.grid(axis='x', linestyle='--', alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nSinais de compra próximos da máxima móvel de 15 dias: {prox_max}")
print(f"Sinais de venda próximos da mínima móvel de 15 dias: {prox_min}")

"""# **Parte 7 - Feature engineering**"""

# Exluindo colunas nao ter caso de vazamento dados()
colunas_para_excluir = ['Date', 'Unnamed: 0', 'Next_Close', 'Movimento', 'Variação_%', 'MA_5', 'MA_15',
                        'MA_5_prev', 'MA_15_prev', 'cruzou_cima', 'cruzou_baixo', 'sinal',
                        'retorno', 'retorno_estrategia', 'max_15', 'min_15', 'ret_1d', 'ret_5d',
                        'vol_5d', 'Close_MA5_ratio', 'cruzamento_MA']

#
data = data.drop(columns=colunas_para_excluir, errors='ignore')

#
data

# Cópia dataset para modelo rede neural - Recorrente
data_train = data.copy()

# Salvando dataset
data_train.to_csv("Dataset_ações_padraão.csv")

# Dataset para modelo rede neural Classficação - Rede neural Binaria (MLP)
data.head()

#

#
bbas_train = pd.read_csv(BBAS3_SA_train, usecols=lambda c: c != 'Unnamed: 0')
bbas_test  = pd.read_csv(BBAS3_SA_test,  usecols=lambda c: c != 'Unnamed: 0')

#
csna_train = pd.read_csv(CSNA3_SA_train, usecols=lambda c: c != 'Unnamed: 0')
csna_test  = pd.read_csv(CSNA3_SA_test,  usecols=lambda c: c != 'Unnamed: 0')

#
petr_train = pd.read_csv(PETR4_SA_train, usecols=lambda c: c != 'Unnamed: 0')
petr_test  = pd.read_csv(PETR4_SA_test,  usecols=lambda c: c != 'Unnamed: 0')

#
vale_train = pd.read_csv(VALE3_SA_train, usecols=lambda c: c != 'Unnamed: 0')
vale_test  = pd.read_csv(VALE3_SA_test,  usecols=lambda c: c != 'Unnamed: 0')

# Visualizando tipos dados
print(bbas_train.dtypes)

"""# Parte 8 - Divisão treino e teste

- Nesta etapa será realizada a divisão dos dados em conjuntos de treino e teste, etapa fundamental para a construção de modelos de redes neurais voltados para tarefas de classificação.


"""

# ==========> VALE3
X_vale = df_train_vale.drop(columns=['Label', 'Date'], errors='ignore')
y_vale = (df_train_vale['Label'] == 1).astype(int)

# ==========> PETR4
X_petr = df_train_petr.drop(columns=['Label', 'Date'], errors='ignore')
y_petr = (df_train_petr['Label'] == 1).astype(int)

# ==========> BBAS3
X_bbas = df_train_bbas.drop(columns=['Label', 'Date'], errors='ignore')
y_bbas = (df_train_bbas['Label'] == 1).astype(int)

# ==========> CSNA3
X_csna = df_train_csna.drop(columns=['Label', 'Date'], errors='ignore')
y_csna = (df_train_csna['Label'] == 1).astype(int)

# Visualizando linhas e y
print("Linhas e colunas x - VALE", X_vale.shape)
print("Linhas e colunas y - VALE", y_vale.shape)

"""# **Parte 9 - Treinamento modelo**"""

# Sempre pegue o y ANTES do split/normalização!
y_vale = (df_train_vale['Label'] == 1).astype(int).values.ravel()    # 0 ou 1, shape (n,)

from sklearn.model_selection import train_test_split

#
X_vale_train, X_vale_test, y_vale_train, y_vale_test = train_test_split(X_vale, y_vale, test_size=0.2, stratify=y_vale, random_state=42)

#
X_petr_train, X_petr_test, y_petr_train, y_petr_test = train_test_split(X_petr, y_petr, test_size=0.2, stratify=y_petr, random_state=42)

#
X_bbas_train, X_bbas_test, y_bbas_train, y_bbas_test = train_test_split(X_bbas, y_bbas, test_size=0.2, stratify=y_bbas, random_state=42)

#
X_csna_train, X_csna_test, y_csna_train, y_csna_test = train_test_split(X_csna, y_csna, test_size=0.2, stratify=y_csna, random_state=42)


# Imprimir shapes para verificar
print("VALE y_train únicos:", np.unique(y_vale_train))  # [0 1]
print("VALE y_test únicos:", np.unique(y_vale_test))    # [0 1]
print("y_vale_train shape:", y_vale_train.shape)         # (n,)

"""# Parte 10 - Normalização dados"""

from sklearn.preprocessing import StandardScaler

# Padroniza features - VALE
scaler_vale = StandardScaler()
X_vale_train_scaled = scaler_vale.fit_transform(X_vale_train)
X_vale_test_scaled = scaler_vale.transform(X_vale_test)

#
scaler_petr = StandardScaler()
X_petr_train_scaled = scaler_petr.fit_transform(X_petr_train)
X_petr_test_scaled = scaler_petr.transform(X_petr_test)

#
scaler_bbas = StandardScaler()
X_bbas_train_scaled = scaler_bbas.fit_transform(X_bbas_train)
X_bbas_test_scaled = scaler_bbas.transform(X_bbas_test)

#
scaler_csna = StandardScaler()
X_csna_train_scaled = scaler_csna.fit_transform(X_csna_train)
X_csna_test_scaled = scaler_csna.transform(X_csna_test)

#
scaler_vale

"""# **Parte 11 - Rede Neural 1 - Classficação**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.callbacks import EarlyStopping

# ==== Funções de arquitetura ====
def build_mlp_1(input_shape):
    model = Sequential([
        Dense(32, activation='relu', input_shape=(input_shape,)),
        Dropout(0.3),
        Dense(1, activation='sigmoid')
    ])
    return model

def build_mlp_2(input_shape):
    model = Sequential([
        Dense(64, activation='relu', input_shape=(input_shape,)),
        BatchNormalization(),
        Dropout(0.4),
        Dense(32, activation='relu'),
        Dropout(0.3),
        Dense(16, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    return model

def build_mlp_3(input_shape):
    model = Sequential([
        Dense(16, activation='tanh', input_shape=(input_shape,)),
        Dense(1, activation='sigmoid')
    ])
    return model

# Modelos Rede Neural
model_builders = {"MLP_Simples": build_mlp_1,
                  "MLP_Profunda": build_mlp_2,
                  "MLP_Shallow": build_mlp_3}

# Seu datasets...
datasets = {
    'VALE3': (X_vale_train, y_vale_train, X_vale_test, y_vale_test, X_vale),
    'PETR4': (X_petr_train, y_petr_train, X_petr_test, y_petr_test, X_petr),
    'BBAS3': (X_bbas_train, y_bbas_train, X_bbas_test, y_bbas_test, X_bbas),
    'CSNA3': (X_csna_train, y_csna_train, X_csna_test, y_csna_test, X_csna),
}

# ==== Flatten se necessário ====
def flatten_if_needed(X):
    if X.ndim == 3:
        return X.reshape(X.shape[0], -1)
    return X

# ==== Mostra o summary de cada arquitetura ====
print("===== SUMMARY DAS ARQUITETURAS =====")
input_shape_exemplo = next(iter(datasets.values()))[0]  # pega o X_train do primeiro ticker
input_shape_exemplo = flatten_if_needed(input_shape_exemplo)
input_dim = input_shape_exemplo.shape[1]

for model_name, builder in model_builders.items():
    print(f"\n---- {model_name} ----")
    model = builder(input_dim)
    model.summary()

# Garante que os dicionários existem
scalers = {}
models = {}
histories = {}

print("\n===== INICIANDO TREINAMENTO =====")

# Loop de treinamento
for ticker, (X_train, y_train, X_test, y_test, X_full) in datasets.items():
    print(f"\nTreinando modelos para {ticker}\n")
    X_train = flatten_if_needed(X_train)
    X_test = flatten_if_needed(X_test)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    scalers[ticker] = scaler

    for model_name, builder in model_builders.items():
        print()
        print(f"   → Arquitetura: {model_name}")
        print()

        model = builder(X_train.shape[1])
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

        history = model.fit(X_train_scaled, y_train,
                            validation_data=(X_test_scaled, y_test),
                            epochs=100,
                            batch_size=64,
                            callbacks=[early_stop],
                            verbose=1)

        models[(ticker, model_name)] = model
        histories[(ticker, model_name)] = history
        val_acc = history.history['val_accuracy'][-1]
        print()
        print(f"      Val_Acc final: {val_acc:.4f}")
        print()

print("\nTreinamento finalizado para todos os modelos e tickers!")

#
for ticker in histories:
    history = histories[ticker].history
    epochs = range(1, len(history['accuracy']) + 1)

    plt.figure(figsize=(14,5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, history['accuracy'], label='Treino')
    plt.plot(epochs, history['val_accuracy'], label='Validação')
    plt.title(f"Acurácia - {ticker}")
    plt.xlabel("Épocas")
    plt.ylabel("Acurácia")
    plt.legend()
    plt.grid(False)

    plt.subplot(1, 2, 2)
    plt.plot(epochs, history['loss'], label='Treino')
    plt.plot(epochs, history['val_loss'], label='Validação')
    plt.title(f"Loss - {ticker}")
    plt.xlabel("Épocas")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(False)

    plt.suptitle(f"Desempenho da Rede Neural - {ticker}", fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(f'history{ticker}.png')
    plt.show()

"""# **Parte 12 - Métricas e avaliaçãoes**"""

from sklearn.metrics import confusion_matrix

# Defina antes!
dados_full = {'VALE3': (X_vale, y_vale),
              'PETR4': (X_petr, y_petr),
              'BBAS3': (X_bbas, y_bbas),
              'CSNA3': (X_csna, y_csna),
              }

for ticker in dados_full:
    for arch in model_builders.keys():

        print(f"\nMatriz de Confusão - {ticker} - {arch}")

        X, y = dados_full[ticker]
        X_scaled = scalers[ticker].transform(X)
        y_pred = (models[(ticker, arch)].predict(X_scaled) >= 0.5).astype(int).flatten()

        cm = confusion_matrix(y, y_pred)


        sns.heatmap(cm, annot=True, cmap="Blues", fmt='d',
                    xticklabels=["Venda", "Compra"],
                    yticklabels=["Venda", "Compra"])

        plt.title(f"Matriz de Confusão - {ticker} - {arch}")
        plt.xlabel("Predito")
        plt.ylabel("Real")
        plt.savefig(f'Matriz_{ticker}_{arch}.png')
        plt.show()

from sklearn.metrics import classification_report

for ticker, (X_train, y_train, X_test, y_test, X_full) in datasets.items():
    print(f"\n Classification Report - {ticker}")

    # X_full é o dataset completo, mas para o y_full use o array correto:
    if ticker == 'VALE3':
        y_full = y_vale
    elif ticker == 'PETR4':
        y_full = y_petr
    elif ticker == 'BBAS3':
        y_full = y_bbas
    elif ticker == 'CSNA3':
        y_full = y_csna

    X_scaled = scalers[ticker].transform(X_full)
    y_pred = (models[(ticker, 'MLP_Simples')].predict(X_scaled) >= 0.5).astype(int).flatten()
    # ajuste o modelo aqui se quiser outra arquitetura

    report = classification_report(y_full, y_pred, target_names=["Venda (-1)", "Compra (+1)"])
    print(report)

from sklearn.metrics import roc_curve, auc

# Dicionário para pegar o y_full correto para cada ticker
y_full_dict = {
    'VALE3': y_vale,
    'PETR4': y_petr,
    'BBAS3': y_bbas,
    'CSNA3': y_csna,
}

for ticker, (X_train, y_train, X_test, y_test, X_full) in datasets.items():
    y_full = y_full_dict[ticker]

    for arch in model_builders.keys():
        print(f"\nCurva ROC - {ticker} - {arch}")

        X_scaled = scalers[ticker].transform(X_full)
        y_proba = models[(ticker, arch)].predict(X_scaled).flatten()

        fpr, tpr, _ = roc_curve(y_full, y_proba)
        roc_auc = auc(fpr, tpr)

        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'Curva ROC - {ticker} - {arch}')
        plt.legend(loc='lower right')
        plt.grid(False)
        plt.tight_layout()
        plt.savefig(f'roc_curve_{ticker}_{arch}.png')
        plt.show()

from sklearn.metrics import f1_score

# Dicionário para pegar o y_full correto para cada ticker
y_full_dict = {'VALE3': y_vale,
               'PETR4': y_petr,
               'BBAS3': y_bbas,
               'CSNA3': y_csna,}

f1_scores = []

for ticker, (X_train, y_train, X_test, y_test, X_full) in datasets.items():
    y_full = y_full_dict[ticker]
    for arch in model_builders.keys():
        print()
        print(f"Calculando F1-score para {ticker} - {arch}")
        print()
        X_scaled = scalers[ticker].transform(X_full)
        y_pred = (models[(ticker, arch)].predict(X_scaled) >= 0.5).astype(int).flatten()

        f1 = f1_score(y_full, y_pred)
        f1_scores.append({'Ticker': ticker,
                          'Arquitetura': arch,
                          'F1-score': round(f1, 4)})

# DataFrame resultados com as ações
df_f1 = pd.DataFrame(f1_scores)
display(df_f1.style.background_gradient(cmap="Blues"))

"""# **Parte 13 - Resultados final Rede Neural Classificação**"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Dicionário com os targets completos
y_full_dict = {
    'VALE3': y_vale,
    'PETR4': y_petr,
    'BBAS3': y_bbas,
    'CSNA3': y_csna,
}

resultados = []

for ticker, (X_train, y_train, X_test, y_test, X_full) in datasets.items():
    y_full = y_full_dict[ticker]
    for arch in model_builders.keys():
        print(f"Avaliando modelo para {ticker} - {arch}")

        X_scaled = scalers[ticker].transform(X_full)

        y_proba = models[(ticker, arch)].predict(X_scaled).flatten()
        y_pred  = (y_proba >= 0.5).astype(int)

        acc  = accuracy_score(y_full, y_pred)
        prec = precision_score(y_full, y_pred, zero_division=0)
        rec  = recall_score(y_full, y_pred)
        f1   = f1_score(y_full, y_pred)
        auc  = roc_auc_score(y_full, y_proba)

        resultados.append({
            'Ticker': ticker,
            'Arquitetura': arch,
            'Accuracy': round(acc, 4),
            'Precision': round(prec, 4),
            'Recall': round(rec, 4),
            'F1-score': round(f1, 4),
            'AUC': round(auc, 4)
        })

# DataFrame
df_resultados = pd.DataFrame(resultados)
display(df_resultados.style.background_gradient(cmap="Blues"))

"""# **Parte 14 - Rede Neural 2 - Série temporal**

- Vamos criar rede neural para empresas como VALE, Petrobras etc. Rede Neural tipo serie temporal com GRU LSTM, CNN, GAN
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, Dropout, LSTM, GRU, Conv1D, Flatten, Conv2D, MaxPooling2D, Reshape
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Dataset geral
bbas = pd.read_csv('/content/BBAS3_SA_treino.csv')
csna = pd.read_csv('/content/CSNA3_SA_treino.csv')
petr = pd.read_csv('/content/PETR4_SA_treino.csv')
vale = pd.read_csv('/content/VALE3_SA_treino.csv')

# VALE3
df_vale_train = pd.read_csv('/content/VALE3_SA_treino.csv')
df_vale_test  = pd.read_csv('/content/VALE3_SA_teste.csv')

# PETR4
df_petr_train = pd.read_csv('/content/PETR4_SA_treino.csv')
df_petr_test  = pd.read_csv('/content/PETR4_SA_teste.csv')

# BBAS3
df_bbas_train = pd.read_csv('/content/BBAS3_SA_treino.csv')
df_bbas_test  = pd.read_csv('/content/BBAS3_SA_teste.csv')

# CSNA3
df_csna_train = pd.read_csv('/content/CSNA3_SA_treino.csv')
df_csna_test  = pd.read_csv('/content/CSNA3_SA_teste.csv')

# Exemplo para VALE3
df_vale_train = df_vale_train.loc[:, ~df_vale_train.columns.str.contains('^Unnamed')]
df_vale_test  = df_vale_test.loc[:, ~df_vale_test.columns.str.contains('^Unnamed')]

print("VALE3:", df_vale_train.columns.tolist())
print("PETR4:", df_petr_train.columns.tolist())

# Função auxiliar para limpar
def limpar_df(df):
    return df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Aplicar nos datasets
df_vale_train = limpar_df(df_vale_train)
df_vale_test  = limpar_df(df_vale_test)

df_petr_train = limpar_df(df_petr_train)
df_petr_test  = limpar_df(df_petr_test)

df_bbas_train = limpar_df(df_bbas_train)
df_bbas_test  = limpar_df(df_bbas_test)

df_csna_train = limpar_df(df_csna_train)
df_csna_test  = limpar_df(df_csna_test)

print("VALE3:", df_vale_train.columns.tolist())
print("PETR4:", df_petr_train.columns.tolist())
print("BBAS3:", df_bbas_train.columns.tolist())
print("CSNA3:", df_csna_train.columns.tolist())

# VALE3
X_vale = df_vale_train[[f'Past_{i}_Days_Close' for i in range(1, 16)]].values
y_vale = df_vale_train['Close'].values

# PETR4
X_petr = df_petr_train[[f'Past_{i}_Days_Close' for i in range(1, 16)]].values
y_petr = df_petr_train['Close'].values

# BBAS3
X_bbas = df_bbas_train[[f'Past_{i}_Days_Close' for i in range(1, 16)]].values
y_bbas = df_bbas_train['Close'].values

# CSNA3
X_csna = df_csna_train[[f'Past_{i}_Days_Close' for i in range(1, 16)]].values
y_csna = df_csna_train['Close'].values

# Escalonamento

# VALE3
scaler_vale_X = StandardScaler()
scaler_vale_y = StandardScaler()
X_vale_scaled = scaler_vale_X.fit_transform(X_vale)
y_vale_scaled = scaler_vale_y.fit_transform(y_vale.reshape(-1, 1))

# PETR4
scaler_petr_X = StandardScaler()
scaler_petr_y = StandardScaler()
X_petr_scaled = scaler_petr_X.fit_transform(X_petr)
y_petr_scaled = scaler_petr_y.fit_transform(y_petr.reshape(-1, 1))

# BBAS3
scaler_bbas_X = StandardScaler()
scaler_bbas_y = StandardScaler()
X_bbas_scaled = scaler_bbas_X.fit_transform(X_bbas)
y_bbas_scaled = scaler_bbas_y.fit_transform(y_bbas.reshape(-1, 1))

# CSNA3
scaler_csna_X = StandardScaler()
scaler_csna_y = StandardScaler()
X_csna_scaled = scaler_csna_X.fit_transform(X_csna)
y_csna_scaled = scaler_csna_y.fit_transform(y_csna.reshape(-1, 1))

#
scaler_vale_X

# Treinamento

# VALE3
split_vale = int(0.8 * len(X_vale_scaled))
X_vale_train, X_vale_test = X_vale_scaled[:split_vale], X_vale_scaled[split_vale:]
y_vale_train, y_vale_test = y_vale_scaled[:split_vale], y_vale_scaled[split_vale:]

# PETR4
split_petr = int(0.8 * len(X_petr_scaled))
X_petr_train, X_petr_test = X_petr_scaled[:split_petr], X_petr_scaled[split_petr:]
y_petr_train, y_petr_test = y_petr_scaled[:split_petr], y_petr_scaled[split_petr:]

# BBAS3
split_bbas = int(0.8 * len(X_bbas_scaled))
X_bbas_train, X_bbas_test = X_bbas_scaled[:split_bbas], X_bbas_scaled[split_bbas:]
y_bbas_train, y_bbas_test = y_bbas_scaled[:split_bbas], y_bbas_scaled[split_bbas:]

# CSNA3
split_csna = int(0.8 * len(X_csna_scaled))
X_csna_train, X_csna_test = X_csna_scaled[:split_csna], X_csna_scaled[split_csna:]
y_csna_train, y_csna_test = y_csna_scaled[:split_csna], y_csna_scaled[split_csna:]

# Adiciona a dimensão extra (features=1)

#
X_vale_train = X_vale_train.reshape((X_vale_train.shape[0], X_vale_train.shape[1], 1))
X_vale_test  = X_vale_test.reshape((X_vale_test.shape[0], X_vale_test.shape[1], 1))

#
X_petr_train = X_petr_train.reshape((X_petr_train.shape[0], X_petr_train.shape[1], 1))
X_petr_test  = X_petr_test.reshape((X_petr_test.shape[0], X_petr_test.shape[1], 1))

#
X_bbas_train = X_bbas_train.reshape((X_bbas_train.shape[0], X_bbas_train.shape[1], 1))
X_bbas_test  = X_bbas_test.reshape((X_bbas_test.shape[0], X_bbas_test.shape[1], 1))

#
X_csna_train = X_csna_train.reshape((X_csna_train.shape[0], X_csna_train.shape[1], 1))
X_csna_test  = X_csna_test.reshape((X_csna_test.shape[0], X_csna_test.shape[1], 1))

"""# Parte 15 - Treinamento Rede Neural LSTM

**Arquiteuras Usadas**

**OBS** **GAN (Generative Adversarial Network)** no contexto de séries temporais financeiras, a GAN pode ser usada para gerar sequências futuras ou aprimorar a previsão.
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, GRU, Flatten, Reshape, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Input, Dense, LeakyReLU
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Conv1D, Conv2D, Flatten, Dense, Dropout, MaxPooling2D

# Parâmetro do espaço latente (ruído)
LATENT_DIM = 15  # igual ao número de features do X (ex: 15)

def build_generator(latent_dim, n_features):
    inp = Input(shape=(latent_dim,))
    x = Dense(32)(inp)
    x = LeakyReLU(0.2)(x)
    x = Dense(64)(x)
    x = LeakyReLU(0.2)(x)
    out = Dense(n_features, activation='linear')(x)
    return Model(inp, out, name='generator')

def build_discriminator(n_features):
    inp = Input(shape=(n_features,))
    x = Dense(64)(inp)
    x = LeakyReLU(0.2)(x)
    x = Dense(32)(x)
    x = LeakyReLU(0.2)(x)
    out = Dense(1, activation='sigmoid')(x)
    return Model(inp, out, name='discriminator')

def compile_gan(generator, discriminator):
    # Discriminator
    discriminator.compile(optimizer=Adam(0.0002), loss='binary_crossentropy', metrics=['accuracy'])
    # GAN
    discriminator.trainable = False
    z = Input(shape=(LATENT_DIM,))
    gen_data = generator(z)
    out = discriminator(gen_data)
    gan = Model(z, out, name='GAN')
    gan.compile(optimizer=Adam(0.0002), loss='binary_crossentropy')
    return gan

#
modelos_dl = ['LSTM', 'GRU', 'CNN1D', 'CNN2D', 'GAN']

# Dicionário com os dados
datasets = {

            #
            'VALE3': (X_vale_train, y_vale_train, X_vale_test, y_vale_test, scaler_vale_y),

            #
            'PETR4': (X_petr_train, y_petr_train, X_petr_test, y_petr_test, scaler_petr_y),

            #
            'BBAS3': (X_bbas_train, y_bbas_train, X_bbas_test, y_bbas_test, scaler_bbas_y),

            #
            'CSNA3': (X_csna_train, y_csna_train, X_csna_test, y_csna_test, scaler_csna_y),
            }

#
resultados = []

#
histories = {}

modelos = {}

# Exemplo de input_shape real para cada arquitetura:
ticker_exemplo = 'VALE3'
X_train_exemplo = datasets[ticker_exemplo][0]
y_train_exemplo = datasets[ticker_exemplo][1]
scaler_y_exemplo = datasets[ticker_exemplo][4]

print("===== SUMMARY DAS ARQUITETURAS DEEP LEARNING =====")

# ---- LSTM ----
print("\n--- LSTM ---")
model_lstm = Sequential()
model_lstm.add(LSTM(32, input_shape=(X_train_exemplo.shape[1], X_train_exemplo.shape[2])))
model_lstm.add(Dropout(0.3))
model_lstm.add(Dense(16, activation='relu'))
model_lstm.add(Dense(1))
model_lstm.summary()

# ---- GRU ----
print("\n--- GRU ---")
model_gru = Sequential()
model_gru.add(GRU(32, input_shape=(X_train_exemplo.shape[1], X_train_exemplo.shape[2])))
model_gru.add(Dropout(0.3))
model_gru.add(Dense(16, activation='relu'))
model_gru.add(Dense(1))
model_gru.summary()

# ---- CNN1D ----
print("\n--- CNN1D ---")
model_cnn1d = Sequential()
model_cnn1d.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_exemplo.shape[1], X_train_exemplo.shape[2])))
model_cnn1d.add(Flatten())
model_cnn1d.add(Dense(16, activation='relu'))
model_cnn1d.add(Dense(1))
model_cnn1d.summary()

# ---- CNN2D ----
print("\n--- CNN2D ---")
X_train_exemplo_2d = X_train_exemplo.reshape((X_train_exemplo.shape[0], X_train_exemplo.shape[1], 1, 1))
model_cnn2d = Sequential()
model_cnn2d.add(Conv2D(16, kernel_size=(3,1), activation='relu', input_shape=(X_train_exemplo_2d.shape[1], 1, 1)))
model_cnn2d.add(MaxPooling2D(pool_size=(2,1)))
model_cnn2d.add(Flatten())
model_cnn2d.add(Dense(16, activation='relu'))
model_cnn2d.add(Dense(1))
model_cnn2d.summary()

# ---- GAN (Generator & Discriminator) ----
# ---- GAN (Generator & Discriminator) ----
input_dim_gan = X_train_exemplo.shape[1] * X_train_exemplo.shape[2] if len(X_train_exemplo.shape) == 3 else X_train_exemplo.shape[1]
print("\n--- GAN Generator ---")
gen = build_generator(LATENT_DIM, input_dim_gan)
gen.summary()

print("\n--- GAN Discriminator ---")
disc = build_discriminator(input_dim_gan)
disc.summary()

from tensorflow.keras.callbacks import EarlyStopping

resultados = []
histories = {}
modelos = {}

for nome_modelo in modelos_dl:   # ['LSTM', 'GRU', 'CNN1D', 'CNN2D', 'GAN']
    for ticker, (X_train, y_train, X_test, y_test, scaler_y) in datasets.items():
        print(f"\nTreinando {nome_modelo} para {ticker}...")

        # =========== Modelos Sequenciais ===========
        if nome_modelo == 'LSTM':
            model = Sequential([LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2])),
                                Dropout(0.3),
                                Dense(16, activation='relu'),
                                Dense(1)])

            model.compile(loss='mean_squared_error', optimizer=Adam(0.001), metrics=['mae'])
            es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

            history = model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=100, batch_size=32, callbacks=[es], verbose=1)
            histories[f"{nome_modelo}_{ticker}"] = history
            modelos[f"{nome_modelo}_{ticker}"] = model

        elif nome_modelo == 'GRU':
            model = Sequential([GRU(32, input_shape=(X_train.shape[1], X_train.shape[2])),
                                Dropout(0.3),
                                Dense(16, activation='relu'),
                                Dense(1)])

            model.compile(loss='mean_squared_error', optimizer=Adam(0.001), metrics=['mae'])
            es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

            history = model.fit(X_train, y_train, validation_data=(X_test, y_test),
                                epochs=100, batch_size=32, callbacks=[es], verbose=1)

            histories[f"{nome_modelo}_{ticker}"] = history
            modelos[f"{nome_modelo}_{ticker}"] = model

        elif nome_modelo == 'CNN1D':
            model = Sequential([Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
                                Flatten(),
                                Dense(16, activation='relu'),
                                Dense(1)])

            model.compile(loss='mean_squared_error', optimizer=Adam(0.001), metrics=['mae'])
            es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
            history = model.fit(X_train, y_train, validation_data=(X_test, y_test),
                                epochs=100, batch_size=32, callbacks=[es], verbose=1)
            histories[f"{nome_modelo}_{ticker}"] = history
            modelos[f"{nome_modelo}_{ticker}"] = model

        elif nome_modelo == 'CNN2D':
            # Ajuste os dados para 4D
            X_train_2d = X_train.reshape((X_train.shape[0], X_train.shape[1], 1, 1))
            X_test_2d = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1))

            model = Sequential([Conv2D(16, kernel_size=(3,1), activation='relu', input_shape=(X_train_2d.shape[1], 1, 1)),
                                MaxPooling2D(pool_size=(2,1)),
                                Flatten(),
                                Dense(16, activation='relu'),
                                Dense(1)])

            model.compile(loss='mean_squared_error', optimizer=Adam(0.001), metrics=['mae'])
            es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

            history = model.fit(X_train_2d, y_train, validation_data=(X_test_2d, y_test),
                                epochs=100, batch_size=32, callbacks=[es], verbose=1)

            histories[f"{nome_modelo}_{ticker}"] = history
            modelos[f"{nome_modelo}_{ticker}"] = model

        # =========== GAN ===========
        elif nome_modelo == 'GAN':
            n_features = X_train.shape[1] if len(X_train.shape) == 2 else X_train.shape[1]*X_train.shape[2]
            X_train_gan = X_train.reshape((X_train.shape[0], n_features))
            X_test_gan = X_test.reshape((X_test.shape[0], n_features))

            generator = build_generator(LATENT_DIM, n_features)
            discriminator = build_discriminator(n_features)
            gan = compile_gan(generator, discriminator)

            epochs = 1000
            batch_size = 64
            half_batch = batch_size // 2

            for epoch in range(epochs):
                idx = np.random.randint(0, X_train_gan.shape[0], half_batch)
                real_samples = X_train_gan[idx]
                noise = np.random.normal(0, 1, (half_batch, LATENT_DIM))
                gen_samples = generator.predict(noise, verbose=0)

                d_loss_real = discriminator.train_on_batch(real_samples, np.ones((half_batch, 1)))
                d_loss_fake = discriminator.train_on_batch(gen_samples, np.zeros((half_batch, 1)))
                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

                noise = np.random.normal(0, 1, (batch_size, LATENT_DIM))
                valid_y = np.ones((batch_size, 1))
                g_loss = gan.train_on_batch(noise, valid_y)

                if epoch % 200 == 0:
                    print(f"GAN {ticker} Epoch {epoch}/{epochs} | D_loss: {d_loss[0]:.4f}, G_loss: {g_loss:.4f}")

            # Exemplo: gera amostras sintéticas
            noise = np.random.normal(0, 1, (X_test_gan.shape[0], LATENT_DIM))
            gen_samples = generator.predict(noise, verbose=0)
            print("Amostra sintética (primeira linha):", gen_samples[0])

            # Você pode guardar o generator, discriminator ou ambos se quiser para análise posterior
            modelos[f"{nome_modelo}_{ticker}_generator"] = generator
            modelos[f"{nome_modelo}_{ticker}_discriminator"] = discriminator

print("\nTreinamento de TODOS os modelos concluído!")

"""# Parte 16 - Métricas e avaliações"""

# Número total de gráficos (um por combinação modelo + ação)
n_graficos = len(histories)
ncols = 2  # uma coluna para Loss, outra para MAE
nrows = n_graficos

for chave, history in histories.items():
    # Tenta dividir a chave corretamente
    if '_' in chave:
        modelo, ticker = chave.split('_', 1)
    else:
        modelo = 'Desconhecido'
        ticker = chave

    hist = history.history
    epochs = range(1, len(hist['loss']) + 1)

    plt.figure(figsize=(14, 5))
    plt.suptitle(f'Desempenho da Rede Neural - {ticker} ({modelo})', fontsize=16)

    # LOSS
    plt.subplot(1, 2, 1)
    plt.plot(epochs, hist['loss'], label='Treino')
    if 'val_loss' in hist:
        plt.plot(epochs, hist['val_loss'], label='Validação')
    plt.title(f'Loss - {ticker}')
    plt.xlabel('Épocas')
    plt.ylabel('Loss')
    plt.legend()

    # MAE
    plt.subplot(1, 2, 2)
    if 'mae' in hist:
        plt.plot(epochs, hist['mae'], label='Treino')
        if 'val_mae' in hist:
            plt.plot(epochs, hist['val_mae'], label='Validação')
        plt.title(f'MAE - {ticker}')
        plt.xlabel('Épocas')
        plt.ylabel('MAE')
        plt.legend()
    else:
        plt.text(0.5, 0.5, 'MAE não disponível', ha='center', va='center', fontsize=12)
        plt.title(f'MAE - {ticker}')

    plt.tight_layout()
    plt.savefig(f'history2{ticker}.png')
    plt.show()

# Considera só os modelos que têm duas partes na chave
modelos_plot = {k: v for k, v in modelos.items() if len(k.split('_')) == 2}

fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(24, 20))
fig.suptitle("📈 Previsão dos Modelos vs. Valores Reais (Close)", fontsize=22)
plt.subplots_adjust(hspace=0.4, wspace=0.3)

for idx, (chave, model) in enumerate(modelos_plot.items()):
    nome_modelo, ticker = chave.split('_')
    X_train, y_train, X_test, y_test, scaler_y = datasets[ticker]

    # Ajustar formato para CNN2D
    if nome_modelo == 'CNN2D':
        X_test_input = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1))
    else:
        X_test_input = X_test

    # Previsões
    y_pred_scaled = model.predict(X_test_input)
    y_pred = scaler_y.inverse_transform(y_pred_scaled)
    y_true = scaler_y.inverse_transform(y_test)

    # Posição do subplot
    row = idx // 4
    col = idx % 4
    ax = axes[row, col]

    # Plot
    ax.plot(y_true, label='Real', linewidth=2)
    ax.plot(y_pred, label='Previsto', linestyle='--')
    ax.set_title(f'{ticker} - {nome_modelo}', fontsize=12)
    ax.set_xlabel("Amostras")
    ax.set_ylabel("Preço Fechamento")
    ax.grid(True)
    ax.legend()

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.savefig("subplots_previsoes_modelos.png", bbox_inches='tight')
plt.show()

resultados = []

for nome_modelo in modelos_dl:
    for ticker, (X_train, y_train, X_test, y_test, scaler_y) in datasets.items():
        # ... treine o modelo aqui ...
        # ... calcule as métricas ...

        resultados.append({
            "Ticker": ticker,
            "Modelo": nome_modelo,
            "MAE": mean_absolute_error(y_true, y_pred),
            "RMSE": np.sqrt(mean_squared_error(y_true, y_pred)),
            "R2": r2_score(y_true, y_pred)
        })
        print(f"Salvo: {ticker}, {nome_modelo}")

#
df_metricas = pd.DataFrame(resultados)
display(df_metricas.style.background_gradient(cmap="Blues"))

"""# **Parte 17 - Previsões Rede Neural 2 - Serie temporal**"""

previsoes = []

for chave, model in modelos.items():
    if '_' not in chave:
        continue  # pula entradas inválidas

    nome_modelo, ticker = chave.rsplit('_', 1)
    if ticker not in datasets:
        continue  # pula modelos GAN generator/discriminator ou chaves erradas

    X_train, y_train, X_test, y_test, scaler_y = datasets[ticker]

    # CNN2D precisa de reshape
    X_test_input = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1)) if nome_modelo == 'CNN2D' else X_test

    y_pred_scaled = model.predict(X_test_input)
    y_pred = scaler_y.inverse_transform(y_pred_scaled)
    y_true = scaler_y.inverse_transform(y_test)

    for i in range(len(y_true)):
        previsoes.append({
            'Ticker': ticker,
            'Modelo': nome_modelo,
            'Amostra': i,
            'Real': y_true[i][0],
            'Previsto': y_pred[i][0]
        })

# Dataset
df_prev = pd.DataFrame(previsoes)

# Salvando dataset
df_prev.to_csv("previsoes_modelos.csv", index=False)

# VIsualizando dataset
display(df_prev.head(n=5).style.background_gradient(cmap="Blues"))

"""# Parte 18 - Previsões base teste

- **Previsões com base de test**
"""

# Caminhos dos arquivos de teste
arquivos_teste = {
    'BBAS3': '/content/BBAS3_SA_teste.csv',
    'CSNA3': '/content/CSNA3_SA_teste.csv',
    'PETR4': '/content/PETR4_SA_teste.csv',
    'VALE3': '/content/VALE3_SA_teste.csv',
}

# Dicionários para armazenar os dados de teste processados
X_teste_final = {}
y_teste_final = {}
scalers_y_test = {}

window_size = 15  # dias anteriores usados

for ticker, caminho in arquivos_teste.items():
    df = pd.read_csv(caminho)

    # Supondo que a coluna alvo é 'Close'
    scaler_x = MinMaxScaler()
    scaler_y = MinMaxScaler()

    # Features e alvo
    features = ['Close']  # adicione mais se usou mais colunas no treino
    dados_x = scaler_x.fit_transform(df[features])
    dados_y = scaler_y.fit_transform(df[['Close']])

    # Criar janelas deslizantes (últimos 15 dias preveem o próximo)
    X, y = [], []
    for i in range(window_size, len(df)):
        X.append(dados_x[i-window_size:i])
        y.append(dados_y[i])

    X = np.array(X)
    y = np.array(y)

    X_teste_final[ticker] = X
    y_teste_final[ticker] = y
    scalers_y_test[ticker] = scaler_y

modelos_validos = ['LSTM', 'GRU', 'CNN1D', 'CNN2D', 'GAN']
avaliacoes_teste = []

for chave, model in modelos.items():
    nome_modelo, ticker = chave.rsplit('_', 1)
    if nome_modelo not in modelos_validos or ticker not in X_teste_final:
        continue

    X_test_novo = X_teste_final[ticker]
    y_test_novo = y_teste_final[ticker]
    scaler_y = scalers_y_test[ticker]

    if nome_modelo == 'CNN2D':
        X_input = X_test_novo.reshape((X_test_novo.shape[0], X_test_novo.shape[1], 1, 1))
    else:
        X_input = X_test_novo

    y_pred_scaled = model.predict(X_input)
    y_pred = scaler_y.inverse_transform(y_pred_scaled)
    y_true = scaler_y.inverse_transform(y_test_novo)

    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    avaliacoes_teste.append({
        "Ticker": ticker,
        "Modelo": nome_modelo,
        "MAE": mae,
        "RMSE": rmse,
        "R2": r2
    })

# Mostrar resultados
df_avaliacoes = pd.DataFrame(avaliacoes_teste)
# VIsualizando dataset
display(df_avaliacoes.style.background_gradient(cmap="Blues"))

fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(24, 20))
fig.suptitle("Previsões dos Modelos nos Dados de Teste vs. Valores Reais (Close)", fontsize=22)
plt.subplots_adjust(hspace=0.4, wspace=0.3)

modelos_plotaveis = []
for chave in modelos:
    nome_modelo, ticker = chave.rsplit('_', 1)
    if nome_modelo in ['LSTM', 'GRU', 'CNN1D', 'CNN2D', 'GAN']:
        modelos_plotaveis.append((chave, modelos[chave]))

for idx, (chave, model) in enumerate(modelos_plotaveis):
    nome_modelo, ticker = chave.rsplit('_', 1)

    X_test = X_teste_final[ticker]
    y_test = y_teste_final[ticker]
    scaler_y = scalers_y_test[ticker]

    if nome_modelo == 'CNN2D':
        X_input = X_test.reshape((X_test.shape[0], X_test.shape[1], 1, 1))
    else:
        X_input = X_test

    y_pred_scaled = model.predict(X_input)
    y_pred = scaler_y.inverse_transform(y_pred_scaled)
    y_true = scaler_y.inverse_transform(y_test)

    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    row = idx // 4
    col = idx % 4
    ax = axes[row, col]

    ax.plot(y_true, label='Real', linewidth=2)
    ax.plot(y_pred, label='Previsto', linestyle='--')
    ax.set_title(f'{ticker} - {nome_modelo}', fontsize=12)
    ax.set_xlabel("Amostras")
    ax.set_ylabel("Preço Fechamento")
    ax.grid(True)
    ax.legend()

    texto_metricas = f"MAE: {mae:.2f}\nRMSE: {rmse:.2f}\nR²: {r2:.2f}"
    ax.text(0.95, 0.05, texto_metricas,
            transform=ax.transAxes,
            verticalalignment='bottom',
            horizontalalignment='right',
            fontsize=10,
            bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.4'))

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.savefig("subplots_previsoes_teste_com_metricas.png", bbox_inches='tight')
plt.show()

"""# Parte 19 - Previsão no futuro (Foresquet)"""

# Número de passos à frente

# Modelo e Ticker (exemplo: modelo LSTM para PETR4)
# 'GRU', 'CNN1D', 'CNN2D'
# Escolher modelo Rede Neural
# Número de passos à frente

# Parâmetros
n_futuro = 30              # Quantidade de dias à frente
ticker   = 'PETR4'         # Escolha: 'VALE3', 'PETR4', 'BBAS3', 'CSNA3'
nome_modelo = 'GAN'        # 'LSTM', 'GRU', 'CNN1D', 'CNN2D', 'GAN'

# ===> Recuperar dados e modelo
X_train, y_train, X_test, y_test, scaler_y = datasets[ticker]

# ===> Gerar previsões futuras
previsoes_futuras = []

if nome_modelo == 'GAN':
    # Para GAN, usar o generator (chave: 'GAN_{ticker}_generator')
    modelo = modelos[f'GAN_{ticker}_generator']
    LATENT_DIM = modelo.input_shape[1]
    for _ in range(n_futuro):
        noise = np.random.normal(0, 1, (1, LATENT_DIM))
        pred_scaled = modelo.predict(noise, verbose=0)
        pred = scaler_y.inverse_transform(pred_scaled)
        previsoes_futuras.append(pred[0, 0])

    # Para plot, utilize o último valor real de teste para juntar as previsões
    y_test_real = scaler_y.inverse_transform(y_test)
    valores_plot = np.concatenate([y_test_real.flatten(), previsoes_futuras])
else:
    # Para LSTM, GRU, CNN1D, CNN2D
    modelo = modelos[f"{nome_modelo}_{ticker}"]
    # Ajustar formato de entrada
    if nome_modelo == 'CNN2D':
        entrada_atual = X_test[-1].reshape(1, X_test.shape[1], 1, 1)
    else:
        entrada_atual = X_test[-1].reshape(1, X_test.shape[1], X_test.shape[2])  # (1, time_steps, features)

    y_test_real = scaler_y.inverse_transform(y_test)
    valores_plot = y_test_real.flatten().tolist()

    for _ in range(n_futuro):
        pred_scaled = modelo.predict(entrada_atual, verbose=0)
        pred = scaler_y.inverse_transform(pred_scaled)
        previsoes_futuras.append(pred[0, 0])
        valores_plot.append(pred[0, 0])
        # Montar próxima entrada (sliding window)
        if nome_modelo == 'CNN2D':
            nova_amostra = pred_scaled.reshape(1, 1, 1, 1)
            entrada_atual = np.concatenate([entrada_atual[:, 1:, :, :], nova_amostra], axis=1)
        else:
            nova_amostra = pred_scaled.reshape(1, 1, 1)
            entrada_atual = np.concatenate([entrada_atual[:, 1:, :], nova_amostra], axis=1)

# ===> Gerar datas para eixo X (supondo DataFrame df com coluna 'Date' em datetime)
len_real = len(y_test_real)
df['Date'] = pd.to_datetime(df['Date'])      # Garante formato datetime
datas_test = df['Date'].iloc[-len_real:].reset_index(drop=True)
ultima_data = datas_test.iloc[-1]
datas_futuras = pd.date_range(start=ultima_data + pd.Timedelta(days=1), periods=n_futuro, freq='B')  # Dias úteis
datas_x = pd.concat([datas_test, pd.Series(datas_futuras)]).reset_index(drop=True)

# Estilo Seaborn
sns.set(style="whitegrid")

plt.figure(figsize=(15, 6))
plt.plot(datas_x[:len_real], valores_plot[:len_real], label='Real', linewidth=2, color='black')
plt.plot(datas_x[len_real-1:], valores_plot[len_real-1:], label='Previsão Futuro', linewidth=2, color='red', linestyle='--', marker='o')
plt.title(f"Previsão {n_futuro} passos à frente - {ticker} - Modelo: {nome_modelo}")
plt.xlabel('Data')
plt.ylabel('Preço Fechamento')
plt.xticks(rotation=45)
plt.legend()
plt.tight_layout()
plt.grid(False)
plt.savefig("Modelo_GAN.png", bbox_inches='tight')
plt.show()

#
n_futuro = 30

#
modelos_dl = ['LSTM', 'GRU', 'CNN1D', 'CNN2D']
cores = ['blue', 'orange', 'green', 'purple']

for ticker in datasets.keys():
    X_train, y_train, X_test, y_test, scaler_y = datasets[ticker]
    y_test_real = scaler_y.inverse_transform(y_test)
    len_real = len(y_test_real)

    # Datas para eixo X
    df['Date'] = pd.to_datetime(df['Date'])
    datas_test = df[df['Ticker'] == ticker]['Date'].iloc[-len_real:].reset_index(drop=True) \
                 if 'Ticker' in df.columns else df['Date'].iloc[-len_real:].reset_index(drop=True)
    ultima_data = datas_test.iloc[-1]
    datas_futuras = pd.date_range(start=ultima_data + pd.Timedelta(days=1), periods=n_futuro, freq='B')
    datas_x = pd.concat([datas_test, pd.Series(datas_futuras)]).reset_index(drop=True)

    plt.figure(figsize=(14, 6))
    plt.plot(datas_x[:len_real], y_test_real.flatten(), label='Real', linewidth=2, color='black')

    for i, nome_modelo in enumerate(modelos_dl):
        chave = f"{nome_modelo}_{ticker}"
        if chave not in modelos:
            continue
        modelo = modelos[chave]
        previsoes_futuras = y_test_real.flatten().tolist()

        # Prepara a entrada para previsão multi-step
        if nome_modelo == 'CNN2D':
            entrada_atual = X_test[-1].reshape(1, X_test.shape[1], 1, 1)
        else:
            entrada_atual = X_test[-1].reshape(1, X_test.shape[1], X_test.shape[2])

        for _ in range(n_futuro):
            pred_scaled = modelo.predict(entrada_atual, verbose=0)
            pred = scaler_y.inverse_transform(pred_scaled)
            previsoes_futuras.append(pred[0, 0])
            # Sliding window
            if nome_modelo == 'CNN2D':
                nova_amostra = pred_scaled.reshape(1, 1, 1, 1)
                entrada_atual = np.concatenate([entrada_atual[:, 1:, :, :], nova_amostra], axis=1)
            else:
                nova_amostra = pred_scaled.reshape(1, 1, 1)
                entrada_atual = np.concatenate([entrada_atual[:, 1:, :], nova_amostra], axis=1)

        plt.plot(datas_x[len_real-1:], previsoes_futuras[len_real-1:],
                 label=f'Previsão {nome_modelo}', linestyle='--', color=cores[i])

    plt.title(f"Previsão dos próximos {n_futuro} dias - {ticker}")
    plt.xlabel("Data")
    plt.ylabel("Preço Fechamento")
    plt.legend()
    plt.xticks(rotation=45)
    plt.grid(False)
    plt.tight_layout()
    plt.savefig("modelos_ANN.png", bbox_inches='tight')
    plt.show()

"""# Parte 20 - Salvando modelos RNN"""

import os
import pickle
import joblib
from tensorflow.keras.models import load_model

PASTA_MODELOS = "modelos_salvos"
os.makedirs(PASTA_MODELOS, exist_ok=True)

# Pasta para armazenar os modelos
os.makedirs('modelos_salvos', exist_ok=True)

# Salvar modelos Keras
for (ticker, model_name), model in models.items():
    caminho = f"modelos_salvos/{ticker}_{model_name}.keras"
    model.save(caminho)
    print()
    print(f"✅ Modelo salvo: {caminho}")
    print()

# Cria a pasta se não existir
os.makedirs('modelos_temporais', exist_ok=True)

for chave, model in modelos.items():
    # Só salva modelos Keras (não salva GAN generator/discriminator duplicados)
    # Se for um Model (Keras) e não apenas um objeto, salva
    if hasattr(model, 'save'):
        nome_arquivo = f"modelos_temporais/{chave}.keras"
        model.save(nome_arquivo)
        print(f"✅ Modelo salvo: {nome_arquivo}")
        print()

# Salvar os scalers de cada ticker
for ticker, scaler_y in [(k, v[4]) for k, v in datasets.items()]:
    nome_arquivo = f"modelos_temporais/scaler_y_{ticker}.pkl"
    with open(nome_arquivo, 'wb') as f:
        pickle.dump(scaler_y, f)
    print(f"✅ Scaler salvo: {nome_arquivo}")
    print()


with open("modelos_temporais/histories.pkl", "wb") as f:
    pickle.dump(histories, f)
print()
print("✅ Histories salvos!")
print()

"""# Parte 18 - Conclusão"""

